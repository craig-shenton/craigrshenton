[
  {
    "objectID": "posts/azure-data-factory-templates.html",
    "href": "posts/azure-data-factory-templates.html",
    "title": "Azure Data Factory Templates",
    "section": "",
    "text": "Open access and reusable design documentation of pipelines used in the NHSX Analytics Azure Data Factory (ADF) environment.\n\nSQL Database Ingestion Pipeline\nDatabricks Ingestion Pipeline\nExcel Sheet Ingestion Pipeline\nMultiple Excel Sheet Ingestion Pipeline\nWeb URL Data Ingestion Pipeline\nAzure Function App Ingestion Pipeline\nSharePoint Ingestion Pipeline\nDatabricks Processing Pipeline\nAzure Function App Processing Pipeline\nMultiple Azure Function Apps Processing Pipeline\nCopy File Processing Pipeline\nSQL Table Staging Pipeline\nMultiple SQL Table Staging Pipeline\n\n\n\n\n\n\n\nNote\n\n\n\nOriginally posted on the NHSX technical gateway website."
  },
  {
    "objectID": "posts/azure-data-factory-templates.html#sql-database-ingestion-pipeline",
    "href": "posts/azure-data-factory-templates.html#sql-database-ingestion-pipeline",
    "title": "Azure Data Factory Templates",
    "section": "SQL Database Ingestion Pipeline",
    "text": "SQL Database Ingestion Pipeline\n\nMetadata\n\nFILE: ingestion_sql.json\nDESCRIPTION: Pipeline to ingest raw data to Azure Datalake blob storage from a SQL database.\nCONTRIBUTORS: Craig Shenton, Mattia Ficarelli\nCONTACT: data@nhsx.nhs.uk\nCREATED: 20 Sept 2021\nVERSION: 0.0.1\n\n\n\nDescription\n\n\n\nData ingestion from a SQL database\n\n\nFigure 1: Data ingestion from a SQL database\nPipeline to ingest raw data to Azure Datalake blob storage from a SQL database.\n\nLooks up the .json configuration file for this pipeline\nSource:\n\nSets the source database owner (dbo)\nSets the source table\nSets the SQL query\n\nSink:\n\nSets the file system\nSets the sink path\nSets the sink file\n\nCopy activity copies the data returned from the SQL query as either a .csv file or a .parquet file.\nIf the copy activity fails, the error notification logic app API will notify the specified email address of the error\n\n\n\nPipeline Configuration\n{\n  \"pipeline\": {\n    \"name\": \"ingestion_sql\",\n    \"folder\": \"templates/ingestion/sql\",\n    \"adl_file_system\": \"file_system\",\n    \"raw\": {\n      \"source_dbo\": \"dbo\",\n      \"source_table\": \"table_1\",\n      \"source_query\": \"SELECT * FROM dbo.table_1 ORDER BY Date DESC\",\n      \"sink_path\": \"raw/path/to/data\",\n      \"sink_file\": \"table_1.parquet\"\n    }\n}\n\n\nData Factory Configuration\nDownload the Azure Data Factory json configuration file to use this template in your own data pipelines.\nsql-ingestion.json"
  },
  {
    "objectID": "posts/azure-data-factory-templates.html#databricks-ingestion-pipeline",
    "href": "posts/azure-data-factory-templates.html#databricks-ingestion-pipeline",
    "title": "Azure Data Factory Templates",
    "section": "Databricks Ingestion Pipeline",
    "text": "Databricks Ingestion Pipeline\n\nMetadata\n\nFILE: ingestion_databricks.json\nDESCRIPTION: Pipeline to ingest raw data to Azure Datalake blob storage using a databricks notebook.\nCONTRIBUTORS: Craig Shenton, Mattia Ficarelli\nCONTACT: data@nhsx.nhs.uk\nCREATED: 20 Sept 2021\nVERSION: 0.0.1\n\n\n\nDescription\n\n\n\nData ingestion using a databricks notebook\n\n\nFigure 2: Data ingestion using a databricks notebook\nPipeline to ingest raw data to Azure Datalake blob storage using a databricks notebook.\n\nLookup the .json configuration file for this pipeline.\nSet the databricks notebook path.\nDatabricks notebook activity runs the databricks notebook specified using an ephemeral job cluster.\nIf the databricks notebook activity fails, the error notification logic app API will notify the specified email address of the error.\n\nWithin the databricks notebook, using Azure Databricks Functions, data can be saved to blob storage as either a .csv file or a .parquet file.\n\n\nPipeline Configuration\n{\n  \"pipeline\": {\n    \"name\": \"ingestion_databricks\",\n    \"folder\": \"templates/ingestion/databricks\",\n    \"adl_file_system\": \"file_system\",\n    \"raw\": {\n      \"databricks_notebook\": \"/path/to/databricks/notebook\"\n    }\n}\n\n\nData Factory Configuration\nDownload the Azure Data Factory .json configuration file to use this template in your own data pipelines.\ndatabricks-ingestion.json"
  },
  {
    "objectID": "posts/azure-data-factory-templates.html#excel-sheet-ingestion-pipeline",
    "href": "posts/azure-data-factory-templates.html#excel-sheet-ingestion-pipeline",
    "title": "Azure Data Factory Templates",
    "section": "Excel Sheet Ingestion Pipeline",
    "text": "Excel Sheet Ingestion Pipeline\n\nMetadata\n\nFILE: ingestion_excel_sheet.json\nDESCRIPTION: Pipeline to ingest a specified excel file sheet, as a .csv file, to Azure Datalake blob storage.\nCONTRIBUTORS: Craig Shenton, Mattia Ficarelli\nCONTACT: data@nhsx.nhs.uk\nCREATED: 20 Sept 2021\nVERSION: 0.0.1\n\n\n\nDescription\n\n\n\nData ingestion of an excel file sheet\n\n\nFigure 3: Data ingestion of an excel file sheet\nPipeline to ingest a specified excel file sheet, as a .csv file, to Azure Datalake blob storage.\n\nLookup the .json configuration file for this pipeline.\nSet the Azure Datalake file system.\nSet the source file path, file name, and excel sheet name.\nSet the sink file path and file name.\nCopy activity ingests the excel sheet data to a .csv file.\nIf the copy activity fails, the error notification logic app API will notify the specified email address of the error.\n\n\n\nPipeline Configuration\n{\n  \"pipeline\": {\n    \"name\": \"ingestion_excel_sheet\",\n    \"folder\": \"templates/ingestion/excel_sheet\",\n    \"adl_file_system\": \"file_system\",\n    \"raw\": {\n      \"source_path\": \"raw/\",\n      \"source_file\": \"file.xlsx\",\n      \"source_sheet\": \"table_1\",\n      \"sink_path\": \"processed/\",\n      \"sink_name\": \"table_1.csv\"\n    }\n}\n\n\nData Factory Configuration\nDownload the Azure Data Factory .json configuration file to use this template in your own data pipelines. excel-sheet-ingestion.json\n\nNote\nAlternatively this a variation of this pipeline can be used to ingest multiple excel file sheets to a set of .csv files in Azure Datalake blob storage."
  },
  {
    "objectID": "posts/azure-data-factory-templates.html#multiple-excel-sheet-ingestion-pipeline",
    "href": "posts/azure-data-factory-templates.html#multiple-excel-sheet-ingestion-pipeline",
    "title": "Azure Data Factory Templates",
    "section": "Multiple Excel Sheet Ingestion Pipeline",
    "text": "Multiple Excel Sheet Ingestion Pipeline\n\nMetadata\n\nFILE: ingestion_multiple_excel_sheets.json\nDESCRIPTION: Pipeline to ingest multiple specified excel file sheets as .csv files to Azure Datalake blob storage.\nCONTRIBUTORS: Craig Shenton, Mattia Ficarelli\nCONTACT: data@nhsx.nhs.uk\nCREATED: 20 Sept 2021\nVERSION: 0.0.1\n\n\n\nDescription\n\n\n\nData ingestion of multiple excel file sheets\n\n\nFigure 4: Data ingestion of multiple excel file sheets\n\n\n\nForEach loop activities within pipeline\n\n\nFigure 5: ForEach loop activities within pipeline\nPipeline to ingest multiple specified excel file sheets as .csv files to Azure Datalake blob storage.\n\nLooks up the .json configuration file for this pipeline.\nSet the Azure Datalake file system.\nSet the source path to the folder containing the excel files.\nSet the sink path.\nSet an array variable containing the list of excel file metadata.\nForEach loops over each excel - FILE:\n\nSets the source sheet and sink file.\nCopy activity ingests the excel sheet data and saves it as a .csv file.\nIf the copy activity fails, the error notification logic app API will notify the specified email address of the error.\n\n\n\nNote\nCopy activity has ‘File path type’ set to wildcard and the file name regex as *.xlsx (excel) (see Figure 6).\n\n\n\nCopy activity wildcard setup\n\n\nFigure 6: Copy activity wildcard setup\n\n\n\nPipeline Configuration\n{\n  \"pipeline\": {\n    \"name\": \"ingestion_multiple_excel_sheets\",\n    \"folder\": \"templates/ingestion/multiple_excel_sheets\",\n    \"adl_file_system\": \"file_system\",\n    \"raw\": {\n      \"source_path\": \"ingestion/\",\n      \"sink_path\": \"raw/path/to/data\",\n      \"sink_path\": \"processed/\"\n      \"excel\":[\n    {\n      \"sink_file\": \"table_1.csv\",\n      \"source_sheet\": \"sheet_1\"\n    },\n    {\n      \"sink_file\": \"table_2.csv\",\n      \"source_sheet\": \"sheet_2\"\n    },\n    {\n      \"sink_file\": \"table_3.csv\",\n      \"source_sheet\": \"sheet_3\"\n    }\n  ]\n}\n\n\nData Factory Configuration\nDownload the Azure Data Factory .json configuration file to use this template in your own data pipelines.\nmultiple-excel-sheet-ingestion.json"
  },
  {
    "objectID": "posts/azure-data-factory-templates.html#web-url-data-ingestion-pipeline",
    "href": "posts/azure-data-factory-templates.html#web-url-data-ingestion-pipeline",
    "title": "Azure Data Factory Templates",
    "section": "Web URL Data Ingestion Pipeline",
    "text": "Web URL Data Ingestion Pipeline\n\nMetadata\n\nFILE: ingestion_web_url.json\nDESCRIPTION: Pipeline to ingest data from a URL as a .csv file to Azure Datalake blob storage.\nCONTRIBUTORS: Craig Shenton, Mattia Ficarelli\nCONTACT: data@nhsx.nhs.uk\nCREATED: 20 Sept 2021\nVERSION: 0.0.1\n\n\n\nDescription\n\n\n\nData ingestion from a web URL\n\n\nFigure 7: Data ingestion from a web URL\nPipeline to ingest data from a web URL as a .csv file to Azure Datalake blob storage.\n\nLookup the .json configuration file for this pipeline.\nSet the source URL.\nSet the file system.\nSet the sink path.\nSet the sink file.\nCopy activity copies the data returned from the URL as a .csv file.\nIf the copy activity fails, the error notification logic app API will notify the specified email address of the error.\n\n\n\nPipeline Configuration\n{\n  \"pipeline\": {\n    \"name\": \"ingestion_web_url\",\n    \"folder\": \"templates/ingestion/web_url\",\n    \"adl_file_system\": \"file_system\",\n    \"raw\": {\n      \"source_url\": \"https://www.sourcedata.com\",\n      \"sink_path\": \"raw/path/to/data\",\n      \"sink_file\": \"table_1.csv\"\n    }\n}\n\n\nData Factory Configuration\nDownload the Azure Data Factory .json configuration file to use this template in your own data pipelines.\nweb-url-ingestion.json"
  },
  {
    "objectID": "posts/azure-data-factory-templates.html#azure-function-app-ingestion-pipeline",
    "href": "posts/azure-data-factory-templates.html#azure-function-app-ingestion-pipeline",
    "title": "Azure Data Factory Templates",
    "section": "Azure Function App Ingestion Pipeline",
    "text": "Azure Function App Ingestion Pipeline\n\nMetadata\n\nFILE: ingestion_function_app.json\nDESCRIPTION: Pipeline to ingest raw data to Azure Datalake blob storage using an Azure function app.\nCONTRIBUTORS: Craig Shenton, Mattia Ficarelli\nCONTACT: data@nhsx.nhs.uk\nCREATED: 29 Sept 2021\nVERSION: 0.0.1\n\n\n\nDescription\n\n\n\nData ingestion using an azure function app\n\n\nFigure 8: Data ingestion using an azure function app\nPipeline to ingest raw data to Azure Datalake blob storage using an Azure function app.\n\nLookup the .json configuration file for this pipeline.\nSet the Azure function app.\nAzure function app activity triggers the specified function app.\nIf the Azure function app activity fails, the error notification logic app API will notify the specified email address of the error.\n\nWithin the Azure function app data can be saved to blob storage as either a .csv file or a .parquet file.\n\n\nPipeline Configuration\n{\n  \"pipeline\": {\n    \"name\": \"ingestion_function_app\",\n    \"folder\": \"templates/ingestion/function_app\",\n    \"adl_file_system\": \"file_system\",\n    \"raw\": {\n      \"func_name\": \"azure_func_app\"\n    }\n}\n\n\nData Factory Configuration\nDownload the Azure Data Factory .json configuration file to use this template in your own data pipelines.\nfunction-app-ingestion.json"
  },
  {
    "objectID": "posts/azure-data-factory-templates.html#sharepoint-ingestion-pipeline",
    "href": "posts/azure-data-factory-templates.html#sharepoint-ingestion-pipeline",
    "title": "Azure Data Factory Templates",
    "section": "SharePoint Ingestion Pipeline",
    "text": "SharePoint Ingestion Pipeline\n\nMetadata\n\nFILE: ingestion_sharepoint.json\nDESCRIPTION: Pipeline to ingest a specified folder and files from Microsoft SharePoint to Azure Datalake blob storage.\nCONTRIBUTORS: Craig Shenton, Mattia Ficarelli\nCONTACT: data@nhsx.nhs.uk\nCREATED: 29 Sept 2021\nVERSION: 0.0.1\n\n\n\nDescription\n\n\n\nData ingestion from microsoft sharepoint\n\n\nFigure 9: Data ingestion from microsoft sharepoint\nPipeline to ingest a specified folder from Microsoft SharePoint to Azure Datalake blob storage.\n\nLookup the .json configuration file for this pipeline.\nSet the SharePoint file path and SharePoint logic app URL.\nCall the SharePoint logic app using a webhook that will send back a message once the file transfer is complete.\nIf the logic app fails, the error notification logic app API will notify the specified email address of the error.\n\n\n\nPipeline Configuration\n{\n  \"pipeline\": {\n    \"name\": \"ingestion_sharepoint\",\n    \"folder\": \"templates/ingestion/sharepoint\",\n    \"adl_file_system\": \"file_system\",\n    \"raw\": {\n      \"source_path\": \"...sharepoint/...\",\n      \"logic_app_url\": \"https://...logic.azure.com/...\"\n    }\n}\n\n\nData Factory Configuration\nDownload the Azure Data Factory .json configuration file to use this template in your own data pipelines.\nsharepoint-ingestion.json"
  },
  {
    "objectID": "posts/azure-data-factory-templates.html#databricks-processing-pipeline",
    "href": "posts/azure-data-factory-templates.html#databricks-processing-pipeline",
    "title": "Azure Data Factory Templates",
    "section": "Databricks Processing Pipeline",
    "text": "Databricks Processing Pipeline\n\nMetadata\n\nFILE: processing_databricks.json\nDESCRIPTION: Pipeline to process data from a folder in Azure Datalake blob storage using a databricks notebook.\nCONTRIBUTORS: Craig Shenton, Mattia Ficarelli\nCONTACT: data@nhsx.nhs.uk\nCREATED: 23 Sept 2021\nVERSION: 0.0.1\n\n\n\nDescription\n\n\n\nData processing using a Databricks notebook\n\n\nFigure 10: Data processing using a Databricks notebook\nPipeline to process data from a folder in Azure Datalake blob storage using a databricks notebook\n\nLookup the .json configuration file for this pipeline.\nSet the databricks notebook path.\nDatabricks notebook activity runs the databricks notebook specified using an ephemeral job cluster.\nIf the databricks notebook activity fails, the error notification logic app API will notify the specified email address of the error.\n\n\n\nPipeline Configuration\n{\n  \"pipeline\": {\n    \"name\": \"processing_databricks\",\n    \"folder\": \"templates/processing/databricks\",\n    \"project\": {\n      \"databricks_notebook\": \"/path/to/databricks/notebook\"\n    }\n}\n\n\nDatabricks Orchestration\n\nNote\nAlternatively this pipeline can be used to trigger an orchestrator databricks notebook which in turn runs a series of data processing notebooks.\n{\n  \"pipeline\": {\n    \"name\": \"processing_databricks\",\n    \"folder\": \"templates/processing/databricks_orchestrator\",\n    \"project\": {\n      \"databricks_orchestrator_notebook\": \"/path/to/databricks/orchestrator_notebook\"\n      \"databricks\":[    \n          {\n        \"sink_path\": \"path/to/processed/data\",\n        \"sink_file\": \"file_1.csv\",\n        \"databricks_notebook\": \"/path/to/databricks/processing_notebook1\"\n        },    \n          {\n        \"sink_path\": \"path/to/processed/data\",\n        \"sink_file\": \"file_2.csv\",\n        \"databricks_notebook\": \"/path/to/databricks/processing_notebook2\"\n        },\n    }\n}\nPython code to sequentially run databricks notebook paths specified in a .json config file from a databricks orchestrator notebook.\n#Squentially run datbricks notebooks\nfor index, item in enumerate(config_JSON['pipeline']['project']['databricks']): \n    notebook = config_JSON['pipeline']['project']['databricks'][index]['databricks_notebook']\n    dbutils.notebook.run(notebook, 120)\n  except Exception as e:\n    print(e)\n\n\n\nData Factory Configuration\nDownload the Azure Data Factory .json configuration file to use this template in your own data pipelines.\nprocessing-databricks.json"
  },
  {
    "objectID": "posts/azure-data-factory-templates.html#azure-function-app-processing-pipeline",
    "href": "posts/azure-data-factory-templates.html#azure-function-app-processing-pipeline",
    "title": "Azure Data Factory Templates",
    "section": "Azure Function App Processing Pipeline",
    "text": "Azure Function App Processing Pipeline\n\nMetadata\n\nFILE: processing_function_app.json\nDESCRIPTION: Pipeline to process data to time-stamped folder in Azure Datalake blob storage using an Azure function app.\nCONTRIBUTORS: Craig Shenton, Mattia Ficarelli\nCONTACT: data@nhsx.nhs.uk\nCREATED: 29 Sept 2021\nVERSION: 0.0.1\n\n\n\nDescription\n\n\n\nData processing using an azure function app\n\n\nFigure 11: Data processing using an azure function app\n\nNote\nThis pipeline is designed to allow for raw data to be ingested and then appended onto an existing table with historical data.\nPipeline to process data to time-stamped folder in Azure Datalake blob storage using an Azure function app.\n\nLookup the .json configuration file for this pipeline.\nSet the source path (of the data to be processed).\nSet the file system.\nSet the Azure function app.\nUse the ‘laterFolder’ utility to find and save the latest folder in the source path.\nIf the ‘laterFolder’ utility fails, the error notification logic app API will notify the specified email address of the error.\nLookup the latest folder.\nSet the latest folder.\nSet the .json body for the Azure function app.\nRun the Azure function app activity.\nIf the Azure function app activity fails, the error notification logic app API will notify the specified email address of the error.\n\nWithin the Azure function app data can be saved to blob storage as either a .csv file or a .parquet file.\n\n\n\nPipeline Configuration\n{\n  \"pipeline\": {\n    \"name\": \"processing_function_app\",\n    \"folder\": \"templates/processing/function_app\",\n    \"adl_file_system\": \"file_system\",\n    \"project\": {\n      \"func_name\": \"azure_func_app\",\n      \"source_path\": \"raw/historical/data/source\"\n    }\n}\n\n\nData Factory Configuration\nDownload the Azure Data Factory .json configuration file to use this template in your own data pipelines.\nfunction-app-processing.json"
  },
  {
    "objectID": "posts/azure-data-factory-templates.html#multiple-azure-function-apps-processing-pipeline",
    "href": "posts/azure-data-factory-templates.html#multiple-azure-function-apps-processing-pipeline",
    "title": "Azure Data Factory Templates",
    "section": "Multiple Azure Function Apps Processing Pipeline",
    "text": "Multiple Azure Function Apps Processing Pipeline\n\nMetadata\n\nFILE: processing_multiple_function_apps.json\nDESCRIPTION: Pipeline to process data to time-stamped folders in Azure Datalake blob storage using multiple Azure function apps.\nCONTRIBUTORS: Craig Shenton, Mattia Ficarelli\nCONTACT: data@nhsx.nhs.uk\nCREATED: 29 Sept 2021\nVERSION: 0.0.1\n\n\n\nDescription\n\n\n\nData processing using multiple azure function apps\n\n\nFigure 12: Data processing using multiple azure function apps\n\n\n\nForEach loop activities within pipeline\n\n\nFigure 13: ForEach loop activities within pipeline\n\nNote\nThis pipeline allows for multiple different processed data files to be generated from the same data source during a pipeline run by using multiple function apps running sequentially.\nPipeline to process data to time-stamped folder in Azure Datalake blob storage using multiple Azure function apps.\n\nLookup the .json configuration file for this pipeline.\nSet the source path (of the data to be processed).\nSet the file system.\nSet the Azure function app.\nUse the ‘laterFolder’ utility to find and save the latest folder in the source path.\nIf the ‘laterFolder’ utility fails, the error notification logic app API will notify the specified email address of the error.\nLookup the latest folder.\nSet the latest folder.\nSet the .json body for the Azure function app.\nSet an array variable containing the list of Azure function apps to be run.\nForEach loops over each azure function: >\n\n\nRuns the Azure function app activity.\nIf the Azure function app activity fails, the error notification logic app API will notify the specified email address of the error.\n\nWithin the Azure function app data can be saved to blob storage as either a .csv file or a .parquet file.\n\n\n\nPipeline Configuration\n{\n  \"pipeline\": {\n    \"name\": \"processing_function_app\",\n    \"folder\": \"templates/processing/function_app\",\n    \"adl_file_system\": \"file_system\",\n    \"project\": {\n      \"functions\": [\n        {\"func_name\": \"azure_func_app_1\"},\n        {\"func_name\": \"azure_func_app_2\"},\n        {\"func_name\": \"azure_func_app_3\"}\n            ],\n      \"source_path\": \"raw/historical/data/source\"\n    }\n}\n\n\nData Factory Configuration\nDownload the Azure Data Factory .json configuration file to use this template in your own data pipelines.\nmultiple-function-app-processing.json"
  },
  {
    "objectID": "posts/azure-data-factory-templates.html#copy-file-processing-pipeline",
    "href": "posts/azure-data-factory-templates.html#copy-file-processing-pipeline",
    "title": "Azure Data Factory Templates",
    "section": "Copy File Processing Pipeline",
    "text": "Copy File Processing Pipeline\n\nMetadata\n\nFILE: processing_csv_file.json\nDESCRIPTION: Pipeline to copy a .csv file in a time-stamped folder between directories in Azure Datalake blob storage.\nCONTRIBUTORS: Craig Shenton, Mattia Ficarelli\nCONTACT: data@nhsx.nhs.uk\nCREATED: 29 Sept 2021\nVERSION: 0.0.1\n\n\n\nDescription\n\n\n\nCopying a .csv file between Azure Datalake directories\n\n\nFigure 14: Copying a .csv file between Azure Datalake directories\nPipeline to copy a .csv file in a time-stamped folder between directories in Azure Datalake blob storage.\n\nLookup the .json configuration file for this pipeline.\nSet the Azure Datalake file system\nSet the source path and source file name.\nSet the sink path and sink file name.\nUse the ‘laterFolder’ utility to find and save the latest folder in the source path.\nIf the ‘laterFolder’ utility fails, the error notification logic app API will notify the specified email address of the error.\nLookup the latest folder.\nSet the latest folder.\nCopy activity copies the .csv file between the Datalake directories.\nIf the copy activity fails, the error notification logic app API will notify the specified email address of the error.\n\n\n\nPipeline Configuration\n{\n  \"pipeline\": {\n    \"name\": \"processing_csv_file\",\n    \"folder\": \"templates/processing/csv_file\",\n    \"adl_file_system\": \"file_system\",\n    \"project\": {\n      \"source_path\": \"raw/\",\n      \"source_name\": \"file.csv\",\n      \"sink_path\": \"proc/\",\n      \"sink_name\": \"file_copy.csv\"\n    }\n}\n\n\nData Factory Configuration\nDownload the Azure Data Factory .json configuration file to use this template in your own data pipelines.\ncsv-file-processing.json"
  },
  {
    "objectID": "posts/azure-data-factory-templates.html#sql-table-staging-pipeline",
    "href": "posts/azure-data-factory-templates.html#sql-table-staging-pipeline",
    "title": "Azure Data Factory Templates",
    "section": "SQL Table Staging Pipeline",
    "text": "SQL Table Staging Pipeline\n\nMetadata\n\nFILE: staging_sql_database.json\nDESCRIPTION: Pipeline to stage data from a time-stamped folder in Azure Datalake blob storage to a table in an Azure SQL database.\nCONTRIBUTORS: Craig Shenton, Mattia Ficarelli\nCONTACT: data@nhsx.nhs.uk\nCREATED: 29 Sept 2021\nVERSION: 0.0.1\n\n\n\nDescription\n\n\n\nData staging to a table in an Azure SQL database\n\n\nFigure 15: Data staging to a table in an Azure SQL database\nPipeline to stage data (.csv file) from a time-stamped folder in Azure Datalake blob storage to a table in an Azure SQL database.\n\nLookup the .json configuration file for this pipeline.\nSet the source path (of data to be staged).\nSet the source file.\nSet the file system.\nSet the sink table (target table in the SQL database).\nSet the stored procedure (truncates data in the target table in the SQL database).\nRun the stored procedure activity. The stored procedure also sets the data type of each column in the database table.\nUse the ‘laterFolder’ utility to find and save the latest folder in the source path.\nIf the ‘laterFolder’ utility fails, the error notification logic app API will notify the specified email address of the error.\nLookup the latest folder.\nSet the latest folder.\nRun the copy activity which stages data from a .csv file in Azure Datalake blob storage to an empty table in an Azure SQL database.\nIf the Azure copy activity fails, the error notification logic app API will notify the specified email address of the error.\n\n\n\nPipeline Configuration\n{\n  \"pipeline\": {\n    \"name\": \"staging_sql_database\",\n    \"folder\": \"templates/staging/sql_database\",\n    \"adl_file_system\": \"file_system\",\n    \"staging\": {\n        \"stored_procedure\":\"[dbo].[sql_stored_procedure_table_1]\",\n        \"source_path\":\"proc/projects/path/to/processed/data/\",\n        \"source_file\":\"table_1.csv\",\n        \"sink_table\":\"sql_table_1\"\n    }\n}\n\n\nData Factory Configuration\nDownload the Azure Data Factory .json configuration file to use this template in your own data pipelines.\nsql-database-staging.json"
  },
  {
    "objectID": "posts/azure-data-factory-templates.html#multiple-sql-table-staging-pipeline",
    "href": "posts/azure-data-factory-templates.html#multiple-sql-table-staging-pipeline",
    "title": "Azure Data Factory Templates",
    "section": "Multiple SQL Table Staging Pipeline",
    "text": "Multiple SQL Table Staging Pipeline\n\nMetadata\n\nFILE: multiple_tables_staging_sql_database.json\nDESCRIPTION: Pipeline to stage data from a time-stamped folders in Azure Datalake blob storage to multiple tables in an Azure SQL database.\nCONTRIBUTORS: Craig Shenton, Mattia Ficarelli\nCONTACT: data@nhsx.nhs.uk\nCREATED: 29 Sept 2021\nVERSION: 0.0.1\n\n\n\nDescription\n\n\n\nData staging to multiple tables in an Azure SQL database\n\n\nFigure 16: Data staging to multiple tables in an Azure SQL database\n\n\n\nForEach loop activities within pipeline\n\n\nFigure 17: ForEach loop activities within pipeline\nPipeline to stage data (.csv files) from a time-stamped folders in Azure Datalake blob storage to multiple tables in an Azure SQL database.\n\nLookup the .json configuration file for this pipeline.\nSet the file system.\nSet an array variable containing the list of stored procedures and tables to which processed data is to be staged.\nFor each element in the list the ForEach loop:\n\nSets the source path (of data to be staged).\nSets the source file.\nUses the ‘laterFolder’ utility to find and save the latest folder in the source path.\nLookups the latest folder.\nSets the latest folder.\nSets the sink table (target table in the SQL database).\nSets the stored procedure (truncates data in the target table in the SQL database).\nRuns the stored procedure activity. The stored procedure also sets the data type of each column in the database table.\nRuns the copy activity which stages data from a .csv file in azure Datalake blob storage to an empty table in an Azure SQL database.\nIf the Azure copy activity fails, the error notification logic app API will notify the specified email address of the error.\n\n\n\n\nPipeline Configuration\n{\n  \"pipeline\": {\n    \"name\": \"multiple_tables_staging_sql_database\",\n    \"folder\": \"templates/staging/multiple_tables_sql_database\",\n    \"adl_file_system\": \"file_system\",\n    \"staging\": [\n          {\n            \"stored_procedure\":\"[dbo].[sql_stored_procedure_table_1]\",\n            \"source_path\":\"proc/projects/path/to/processed/data/\",\n            \"source_file\":\"table_1.csv\",\n            \"sink_table\":\"sql_table_1\"\n          },\n          {\n            \"stored_procedure\":\"[dbo].[sql_stored_procedure_table_2]\",\n            \"source_path\":\"proc/projects/path/to/processed/data2/\",\n            \"source_file\":\"table_2.csv\",\n            \"sink_table\":\"sql_table_2\"\n          },\n          {\n            \"stored_procedure\":\"[dbo].[sql_stored_procedure_table_3]\",\n            \"source_path\":\"proc/projects/path/to/processed/data3/\",\n            \"source_file\":\"table_3.csv\",\n            \"sink_table\":\"sql_table_3\"\n          }\n      ]\n}\n\n\nData Factory Configuration\nDownload the Azure Data Factory .json configuration file to use this template in your own data pipelines.\nmultiple-tables-sql-database-staging.json"
  },
  {
    "objectID": "posts/azure-data-engineering-principles.html",
    "href": "posts/azure-data-engineering-principles.html",
    "title": "Azure Data Engineering Principles",
    "section": "",
    "text": "Our principles are broken down into the following sections:\n\nAzure Data Engineering Principles\n\nParameterisation\n\nExample: latestFolder\n\nConfiguration-as-code\nStandardised ETL Design Patterns\n\nExample 1: SQL Database Ingestion Pipeline\nExample 2: Databricks Processing Pipeline\n\nHierarchical Pipeline Orchestration\nDocumentation-as-code\nReferences\n\n\n\n\n\n\n\n\nNote\n\n\n\nOriginally posted on the NHSX technical gateway website."
  },
  {
    "objectID": "posts/azure-data-engineering-principles.html#parameterisation",
    "href": "posts/azure-data-engineering-principles.html#parameterisation",
    "title": "Azure Data Engineering Principles",
    "section": "Parameterisation",
    "text": "Parameterisation\nIn straightforward copy activities, hard coding each activity’s file paths is easy enough. In Azure Data Factory (ADF), this requires creating a new dataset object for each sink and for each source. Like many users, we initially created new datasets at every stage of our Extract, Transform, and Load (ETL) pipelines. However, once these processes started to scale in complexity to include iteration and conditionals, the sheer amount of datasets and variables that were required to run our pipelines became unmanageable.\nThe first step in untangling this web of configurations is applying parameterisation to your data pipelines. This adds a layer of abstraction to ADF that can dramatically reduce the amount of complexity needed to handle a multitude of ETL processes. Parameterisation transforms your activities into something akin to a function in python that accepts a certain set of variables and arguments. Much like in python, this abstraction allows you to use and re-use the parameterised dataset for all processes of the same type, reducing the need to create a new dataset for each process.\nFor example, we created a generic dataset for handling .csv files on our Azure Datalake that passes the following parameters at runtime:\n\n\n\nFigure 1. An Azure Data Factory dataset file path configuration using the parameters; @dataset().fileSystem, @dataset().filePath, @dataset().fileName to denote the datalake file system name, the file path and and the file name.\n\n\nFrom these parameters, that specify the file path and name and the file system of the Azure Datalake linked service, we can use any .csv file available as the source for any pipeline activity. This has reduced the number of datasets listed in our ADF environment dramatically, reducing the overhead required to organise, search, and maintain our pipelines.\nA downside of highly parameterised pipelines is that they can become harder to debug due to the new level of abstraction. Now, in addition to the file paths the parameters may also be incorrectly configured. However, we find that the reduction in complexity and centralisation of pipeline configuration outweighs the initial growing pains of parameterisation.\n\nExample: latestFolder\n\n\n\nFigure 2. An example Azure Data Factory pipeline utility that can append the source path of any file with the latest time-stamped folder path.\n\n\nA practical example of the utility of parameterisation is the ability to append the source path of any file with a time-stamped folder, for example @concat(variables('sourcePath'),variables('latestFolder')). This allows for a well organised record of data sampled at different time points to be stored within the Datalake."
  },
  {
    "objectID": "posts/azure-data-engineering-principles.html#configuration-as-code",
    "href": "posts/azure-data-engineering-principles.html#configuration-as-code",
    "title": "Azure Data Engineering Principles",
    "section": "Configuration-as-code",
    "text": "Configuration-as-code\nConfiguration-as-code is the practice of managing the configuration of software using plain text .json files stored within a git repository. [2].\nThese ‘config’ files establish the parameters and settings for all of the datasets, linked services, and stored procedures required for a particular ETL pipeline. These files are called via ADF Lookup activities with values set as variables to give ADF everything it needs to know for a pipeline to run end-to-end. This approach means that in order to deploy a whole new data pipeline in ADF, only a new configuration file is required. Thus, in addition to making it easier and quicker to create a new ETL pipeline, maintaining configurations is also centralised, making configuration mismatches between activities easier to avoid, allowing for more consistent deployments.\nData Engineers often store their configurations in a database, alongside the data for convenience, however using structured .json files has additional advantages that should be considered:\n\nThe first is that as plain text files, they can be saved in a git repository, thus putting them under version control. This gives you a level of traceability in terms of how-and-when changes were made and allows for your configurations to go through the same DevOps best practices and code review before they are deployed to production [2].\nThe second benefit is that keeping configuration-as-code separates out your pipeline and configuration deployments. Decoupling these processes allows you to release and/or roll-back changes separately, which is important for tracing and debugging errors. Critically, this allows you to rapidly determine if the returned error is due to a configuration issue or a pipeline coding issue [3]."
  },
  {
    "objectID": "posts/azure-data-engineering-principles.html#standardised-etl-design-patterns",
    "href": "posts/azure-data-engineering-principles.html#standardised-etl-design-patterns",
    "title": "Azure Data Engineering Principles",
    "section": "Standardised ETL Design Patterns",
    "text": "Standardised ETL Design Patterns\nTemplates help us avoid building the same workflows repeatedly, as once developed and thoroughly tested, they can be used in many different pipelines. There are a growing number of common ETL templates available in ADF that are a great resource to get you started, found on Microsoft’s Azure documentation site [4].\nHowever, these templates still need to be hand configured for your specific pipeline. Applying the parameterisation and configuration-as-code principles outlined above to our templates allows us to go much further. We have created a set of fully abstract and perameratised ETL workflows that only require a configuration file lookup to run. In essence, these templates become ‘plug-and-play’, and can be chained together very quickly. By focusing on just a handful of generic and reusable templates, more resources can be allocated to testing and maintaining these resources, knowing that they will be used over and over, by many members of the development team. This is a far more efficient use of development time, and allows us to be confident that new pipelines will run upon their first implementation without much issue. Like the other components in ADF, our template files are simply stored as a JSON file within our code repository, so they can be shared and improved upon by the wider data engineering community.\nFor our internal analytics data engineering work, we have found it useful to break the templates into the following categories:\n\nIngestion: In the first instance we developed ingestion templates for every scenario, allowing us to rapidly ingest new datasets with minimal configuration. These typically involve HTTP requests, API calls, SQL stored procedures, and processes to copy files from SharePoint.\nProcessing: Our analytical processing is largely done through databricks, so these pipelines configure the analytics notebook and start a new spark job cluster.\nStaging: Staging is where we push data to our Tableau SQL server, so we have templates to run multiple stored procedures and update metric tables for each of our analytical products.\nUtilities: Last but by no means least, these are smaller functions that can be called multiple times at any stage of an ETL pipeline. Most involve sending data back and forth to systems outside ADF and/or updating configuration files.\n\n\nExample 1: SQL Database Ingestion Pipeline\n\n\n\nFigure 3. An example Azure Data Factory ingestion pipeline template that can be configured to extract data from an Azure SQL database to Azure Datalake blob storage.\n\n\nThe pipeline shown above is a fully parameterised template developed to ingest raw data from an Azure SQL database to Azure Datalake blob storage. This is an example of a ‘Source-Sink’ pattern–used for parameterising and configuring data copy activities that move data from one location to another. As mentioned in the parameterisation section, each Azure Data Factory copy activity requires at least two datasets to configure both source and sink locations. However, here we have created a generic SQL dataset and a generic Azure Datalake dataset that can be dynamically configured across all pipelines. As such, this template can be used and re-used to move data from any Azure SQL server to any Azure blob storage container.\nThe SQL ingestion template works as follows:\n\nThe configuration file is called using a lookup activity and the resulting configuration values are saved as variables before executing the copy activity at runtime.\nFor the source dataset, we require the parameters for connecting to an Azure SQL server.\nThe server details and connection credentials are passed via an ADF linked service, which itself can be further parameterised.\nThe configuration file then sets the source database owner (dbo) string, the source table name, and if required, a SQL query to filter the data before the copy activity is run.\nOn the sink side, the Datalake connection string is also passed via an ADF linked service, but the file system, sink path, and file name are all set by the configuration file. This could be a .csv file or a .parquet file for example\nFinally, if the copy activity fails for some reason, an error notification pipeline is called that contains a simple logic app used to notify a specified email address of the error [5].\n\n\n\nExample 2: Databricks Processing Pipeline\n\n\n\nFigure 4. An example Azure Data Factory pipeline processing pipeline template that can be configured to run a Databricks notebook.\n\n\nThe parameterised pipeline template above has been developed to run a Databricks notebook from Azure Data Factory. This is an example of a ‘Key-Value’ pattern–useful for configuring the settings of activities outside of Azure Data Factory itself. Here the json configuration file is providing the key-value of a Databricks notebook file path. This could also be used to give the URL of an Azure logic app or pass multiple variables to an Azure function app for example.\nThe Databricks processing template works as follows:\n\nThe configuration file is called using a lookup activity and the resulting configuration values are saved as variables before executing the copy activity at runtime.\nA Set variable activity reads the databricks notebook path and saves it as a Azure Data Factory variable.\nThe Databricks notebook activity runs the specified Databricks notebook using an ephemeral job cluster (therefore no cluster ID is required).\nFinally, if the Databricks notebook activity fails, an error notification pipeline is called that contains a simple logic app used to notify a specified email address of the error.\n\nWe typically use this pipeline to trigger an orchestrator Databricks notebook which in turn runs a series of data processing notebooks. This allows for much more flexibility, as we may want to process multiple metric calculations from the same data source."
  },
  {
    "objectID": "posts/azure-data-engineering-principles.html#hierarchical-pipeline-orchestration",
    "href": "posts/azure-data-engineering-principles.html#hierarchical-pipeline-orchestration",
    "title": "Azure Data Engineering Principles",
    "section": "Hierarchical Pipeline Orchestration",
    "text": "Hierarchical Pipeline Orchestration\n\n\n\nFigure 5.A hierarchicy of pipelines. At the top, a orchestration pipeline that triggers the sub-pipelines below. Each phase of the data processing (extract, transform, and load, or ETL) is a fully parameterised template that requires no setup of its own.\n\n\nOne of the most significant changes made to our data engineering setup is the use of hierarchical pipelines; that is the use of a single orchestration pipeline to trigger multiple ETL pipelines and utility sub-pipelines. This is based on Paul Andrews’ grandparent, parent, child design pattern that may be familiar to SSIS users [9]. With this design, and the generic ETL templates outlined above, we can create a ‘plug-and-play’ data engineering system. We simply select the ingestion, transformation, and staging patterns required from the templates and link them together under the orchestration pipeline. A single json config file is then created with all the ‘Sink-Source’ and ‘Key-Value’ pairs needed to move the data through the ETL process, so no configuration is required in Azure Data Factory itself. This has significantly reduced the amount of time needed to set up new pipelines, and ensures best practices are maintained across all our products.\nIf we use the SQL ingestion and Databricks processing examples outlined above, we could very rapidly make a new ETL pipeline using the hierarchical system of pipeline development:\n\nFirst we would create an orchestration pipeline that would lookup the configuration file and attach any Azure Data Factory triggers that set when, and how often the pipeline would run.\nNext we can simply ‘plug-and-play’ with the ETL templates, first to ingest new data from a SQL server, then process that data with a Databricks notebook.\nFinally, the processed data could be staged, on a Tableau server for example, for BI developers to transform into visualisations and metrics.\nWithin each ETL pipeline there would also be running pre-defined utility sub-pipelines, such as any testing and error handling processes, or our latestFolder example, which could be used to make sure we are processing the latest cut of the data before handing over to Databricks."
  },
  {
    "objectID": "posts/azure-data-engineering-principles.html#documentation-as-code",
    "href": "posts/azure-data-engineering-principles.html#documentation-as-code",
    "title": "Azure Data Engineering Principles",
    "section": "Documentation-as-code",
    "text": "Documentation-as-code\nDocumentation-as-code is the principle that either; documentation should be written with the same tools as your code, or that documentation should be automatically generated from your code [10]. On a basic level this means that we manage our documentation via GitHub, following a Git workflow, and putting it under version control in the same way as our configuration files. In addition to the standard GitFlow best practices, we can also compare versions of both the product and the documentation, making it easier to see where one might be out of sync with the other. Like much of our work, we make the documentation repository open source to increase transparency and allow for others in the healthcare sector to implement our data engineering principles and best practice. Due to the structured nature of the .json format, our pipeline configuration files and the configuration files generated from ADF are readily transformed into tables and diagrams using python. As a result, we can automatically generate a great deal of our documentation and make sure it directly represents the live product."
  },
  {
    "objectID": "posts/azure-data-engineering-principles.html#references",
    "href": "posts/azure-data-engineering-principles.html#references",
    "title": "Azure Data Engineering Principles",
    "section": "References",
    "text": "References\n[1] NHSX (2021). Data Engineering Documentation Site: ADF Utilities - latestFolder [Online] https://nhsx.github.io/au-data-engineering/adfutilities.html#latest-folder-lookup\n[2] Perforce (2020). Configuration as Code: How to Streamline Your Pipeline. [Online] https://www.perforce.com/blog/vcs/configuration-as-code\n[3] Cloud Bees (2018). Configuration as Code: Everything You Need to Know. [Online] https://www.cloudbees.com/blog/configuration-as-code-everything-need-know\n[4] Microsoft (2021). Azure Data Factory documentation site [Online] https://docs.microsoft.com/en-us/azure/data-factory/solution-templates-introduction\n[5] MSSQL Tips (2019). Azure Data Factory Pipeline Email Notification. [Online] https://www.mssqltips.com/sqlservertip/5718/azure-data-factory-pipeline-email-notification-part-1/\n[6] NHSX (2021). Data Engineering Documentation Site: SQL Database Ingestion Pipeline [Online] https://nhsx.github.io/au-data-engineering/adfpipelines.html#sql-database-ingestion-pipeline\n[7] NHSX (2021). Data Engineering Documentation Site: Databricks Processing Pipeline [Online] https://nhsx.github.io/au-data-engineering/adfpipelines.html#databricks-processing-pipeline\n[8] Paul Andrews (2019). The icons used for the hierarchical pipeline orchestration section of this post were designed by Paul Andrews. [Online] https://github.com/mrpaulandrew/ContentCollateral\n[9] Paul Andrews (2019). Azure Data Factory Pipeline Hierarchies (Generation Control) [Online] https://mrpaulandrew.com/2019/09/25/azure-data-factory-pipeline-hierarchies-generation-control/]\n[10] GOV.UK Technology in Government (2017). Why we use a ‘docs as code’ approach for technical documentation. [Online] https://technology.blog.gov.uk/2017/08/25/why-we-use-a-docs-as-code-approach-for-technical-documentation/\nChris Ried 2018. Cover photo from Unsplash. [Online] https://unsplash.com/s/photos/python?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Craig Robert Shenton",
    "section": "",
    "text": "Azure Data Factory Templates\n\n\n\n\nOpen access and reusable design documentation of pipelines used in the NHSX Analytics Azure Data Factory (ADF) environment.\n\n\n\n\n\n\n9/9/21\n\n\n\n\n\n\n\n\nAzure Data Engineering Principles\n\n\n\n\nIn this post, we aim to set out the data engineering principles NHSX has developed in designing and building our Azure cloud-analytics infrastructure.\n\n\n\n\n\n\n9/1/21\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "drafts/azure-data-factory-resources.html",
    "href": "drafts/azure-data-factory-resources.html",
    "title": "Azure Data Factory Resources",
    "section": "",
    "text": "Transforming JSON to CSV with the help of Flatten task in Azure Data Factory\n{:class=“img-responsive”}\nTransform/transpose/flatten your JSON structure into a denormalized flatten datasets that you can upload into a new or existing flat database table.\nHow to read files on SharePoint Online using Azure Data Factory. Organizations that have moved to Office 365, maintain few key data points on excel files/SharePoint Lists stored in SharePoint Online. If the situation demands you to analyze these data points, it has to be consumed to a database or a data lake.\nAzure Data Factory Pipeline Email Notification – Part 1. When building ETL pipelines, you typically want to notify someone when something goes wrong (or when everything has finished successfully). Usually this is done by sending an e-mail to the support team or someone else who is responsible for the ETL. But in Azure Data Factory there’s no built-in activity for sending an e-mail. Here is an implementation using the Web Activity and an Azure Logic App."
  },
  {
    "objectID": "drafts/data_eng_list.html",
    "href": "drafts/data_eng_list.html",
    "title": "Data Engineering Research",
    "section": "",
    "text": "A. Relational databases (SQL)\nB. NoSQL databases\nC. Time-series databases\nD. Graph databases\n\n\n\n\n\n\n\n\nA. Star schema\nB. Snowflake schema"
  },
  {
    "objectID": "drafts/data_eng_list.html#etl-extract-transform-load-process",
    "href": "drafts/data_eng_list.html#etl-extract-transform-load-process",
    "title": "Data Engineering Research",
    "section": "ETL (Extract, Transform, Load) process",
    "text": "ETL (Extract, Transform, Load) process"
  },
  {
    "objectID": "drafts/data_eng_list.html#elt-extract-load-transform-process",
    "href": "drafts/data_eng_list.html#elt-extract-load-transform-process",
    "title": "Data Engineering Research",
    "section": "ELT (Extract, Load, Transform) process",
    "text": "ELT (Extract, Load, Transform) process"
  },
  {
    "objectID": "drafts/data_eng_list.html#data-pipelines",
    "href": "drafts/data_eng_list.html#data-pipelines",
    "title": "Data Engineering Research",
    "section": "Data pipelines",
    "text": "Data pipelines"
  },
  {
    "objectID": "drafts/data_eng_list.html#data-ingestion-methods",
    "href": "drafts/data_eng_list.html#data-ingestion-methods",
    "title": "Data Engineering Research",
    "section": "Data ingestion methods",
    "text": "Data ingestion methods\n\nA. Batch processing\nB. Stream processing"
  },
  {
    "objectID": "drafts/data_eng_list.html#data-transformation",
    "href": "drafts/data_eng_list.html#data-transformation",
    "title": "Data Engineering Research",
    "section": "Data transformation",
    "text": "Data transformation\n\nA. Data cleaning\nB. Data enrichment\nC. Data validation"
  },
  {
    "objectID": "drafts/data_eng_list.html#data-orchestration-tools",
    "href": "drafts/data_eng_list.html#data-orchestration-tools",
    "title": "Data Engineering Research",
    "section": "Data orchestration tools",
    "text": "Data orchestration tools\n\nA. Apache Airflow\nB. Prefect"
  },
  {
    "objectID": "drafts/data_eng_list.html#apache-spark",
    "href": "drafts/data_eng_list.html#apache-spark",
    "title": "Data Engineering Research",
    "section": "Apache Spark",
    "text": "Apache Spark"
  },
  {
    "objectID": "drafts/data_eng_list.html#data-partitioning-and-sharding",
    "href": "drafts/data_eng_list.html#data-partitioning-and-sharding",
    "title": "Data Engineering Research",
    "section": "Data partitioning and sharding",
    "text": "Data partitioning and sharding"
  },
  {
    "objectID": "drafts/data_eng_list.html#streaming-data-processing",
    "href": "drafts/data_eng_list.html#streaming-data-processing",
    "title": "Data Engineering Research",
    "section": "Streaming data processing",
    "text": "Streaming data processing\n\nA. Apache Kafka"
  },
  {
    "objectID": "drafts/data_eng_list.html#data-encryption",
    "href": "drafts/data_eng_list.html#data-encryption",
    "title": "Data Engineering Research",
    "section": "Data encryption",
    "text": "Data encryption"
  },
  {
    "objectID": "drafts/data_eng_list.html#data-anonymization-and-masking",
    "href": "drafts/data_eng_list.html#data-anonymization-and-masking",
    "title": "Data Engineering Research",
    "section": "Data anonymization and masking",
    "text": "Data anonymization and masking"
  },
  {
    "objectID": "drafts/data_eng_list.html#data-lineage",
    "href": "drafts/data_eng_list.html#data-lineage",
    "title": "Data Engineering Research",
    "section": "Data lineage",
    "text": "Data lineage"
  },
  {
    "objectID": "drafts/data_eng_list.html#data-cataloging",
    "href": "drafts/data_eng_list.html#data-cataloging",
    "title": "Data Engineering Research",
    "section": "Data cataloging",
    "text": "Data cataloging"
  },
  {
    "objectID": "drafts/data_eng_list.html#compliance-and-regulations",
    "href": "drafts/data_eng_list.html#compliance-and-regulations",
    "title": "Data Engineering Research",
    "section": "Compliance and regulations",
    "text": "Compliance and regulations\n\nA. GDPR (General Data Protection Regulation)"
  },
  {
    "objectID": "drafts/data_eng_list.html#data-quality-dimensions",
    "href": "drafts/data_eng_list.html#data-quality-dimensions",
    "title": "Data Engineering Research",
    "section": "Data quality dimensions",
    "text": "Data quality dimensions"
  },
  {
    "objectID": "drafts/data_eng_list.html#data-profiling",
    "href": "drafts/data_eng_list.html#data-profiling",
    "title": "Data Engineering Research",
    "section": "Data profiling",
    "text": "Data profiling"
  },
  {
    "objectID": "drafts/data_eng_list.html#data-quality-tools",
    "href": "drafts/data_eng_list.html#data-quality-tools",
    "title": "Data Engineering Research",
    "section": "Data quality tools",
    "text": "Data quality tools"
  },
  {
    "objectID": "drafts/data_eng_list.html#data-monitoring-and-alerting",
    "href": "drafts/data_eng_list.html#data-monitoring-and-alerting",
    "title": "Data Engineering Research",
    "section": "Data monitoring and alerting",
    "text": "Data monitoring and alerting"
  },
  {
    "objectID": "drafts/data_eng_list.html#scalability-and-performance-optimization",
    "href": "drafts/data_eng_list.html#scalability-and-performance-optimization",
    "title": "Data Engineering Research",
    "section": "Scalability and performance optimization",
    "text": "Scalability and performance optimization"
  },
  {
    "objectID": "drafts/data_eng_list.html#fault-tolerance-and-reliability",
    "href": "drafts/data_eng_list.html#fault-tolerance-and-reliability",
    "title": "Data Engineering Research",
    "section": "Fault tolerance and reliability",
    "text": "Fault tolerance and reliability"
  },
  {
    "objectID": "drafts/data_eng_list.html#data-modeling",
    "href": "drafts/data_eng_list.html#data-modeling",
    "title": "Data Engineering Research",
    "section": "Data modeling",
    "text": "Data modeling"
  },
  {
    "objectID": "drafts/data_eng_list.html#continuous-integration-and-continuous-deployment-cicd",
    "href": "drafts/data_eng_list.html#continuous-integration-and-continuous-deployment-cicd",
    "title": "Data Engineering Research",
    "section": "Continuous integration and continuous deployment (CI/CD)",
    "text": "Continuous integration and continuous deployment (CI/CD)\n\nIntroduction to CI/CD\n\nA. Definition and benefits\nB. CI/CD in data engineering projects\nC. CI/CD tools and platforms\n\n\n\nContinuous Integration (CI)\n\nA. Version control systems\nB. Automated build systems\nC. Automated testing in CI\nD. Code review and quality checks\n\n\n\nContinuous Deployment (CD)\n\nA. Deployment strategies\nB. Infrastructure as Code (IaC)\nC. Containerization and orchestration\nD. Deployment automation\n\n\n\nMonitoring and observability in CI/CD\n\nA. Monitoring tools integration\nB. Log management and analysis\nC. Performance monitoring\n\n\n\nCI/CD best practices for data engineering\n\nA. Incremental and iterative development\nB. Configuration management\nC. Secure and reliable deployments\n\n\n\nCase studies and success stories\n\nA. Real-world CI/CD implementations\nB. Lessons learned and challenges faced"
  },
  {
    "objectID": "drafts/data_eng_list.html#testing-and-validation",
    "href": "drafts/data_eng_list.html#testing-and-validation",
    "title": "Data Engineering Research",
    "section": "Testing and validation",
    "text": "Testing and validation\n\nUnit testing for data pipelines\n\nA. Test individual components\nB. Mocking external dependencies\nC. Isolation of test environments\n\n\n\nIntegration testing for data pipelines\n\nA. Test end-to-end data flow\nB. Validate data transformations and integrations\nC. Testing with realistic data sets\n\n\n\nData validation\n\nA. Schema validation\nB. Data type validation\nC. Range and constraint validation\nD. Uniqueness and referential integrity validation\n\n\n\nPerformance testing\n\nA. Load testing\nB. Stress testing\nC. Benchmarking\n\n\n\nData quality testing\n\nA. Data accuracy and consistency\nB. Data completeness\nC. Data timeliness\nD. Data lineage verification\n\n\n\nRegression testing\n\nA. Test existing data pipelines for backward compatibility\nB. Validate new changes against old functionality\n\n\n\nTest automation\n\nA. Continuous testing in CI/CD pipelines\nB. Automated test generation\nC. Test execution and reporting\n\n\n\nMonitoring and alerting for test failures\n\nA. Integration with monitoring tools\nB. Anomaly detection\nC. Notification and escalation strategies\n\n\n\nTest data management\n\nA. Synthetic test data generation\nB. Test data masking and anonymisation\nC. Test data storage and versioning\n\n\n\nBest practices for testing and validation\n\nA. Test-driven development (TDD) in data engineering\nB. Code and test coverage metrics\nC. Testing strategies and test pyramid"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Craig Robert Shenton",
    "section": "",
    "text": "Hi 👋, I’m Craig."
  }
]