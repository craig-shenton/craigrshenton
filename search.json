[
  {
    "objectID": "posts/azure-data-factory-templates.html",
    "href": "posts/azure-data-factory-templates.html",
    "title": "Azure Data Factory Templates",
    "section": "",
    "text": "Open access and reusable design documentation of pipelines used in the NHSX Analytics Azure Data Factory (ADF) environment.\n\nSQL Database Ingestion Pipeline\nDatabricks Ingestion Pipeline\nExcel Sheet Ingestion Pipeline\nMultiple Excel Sheet Ingestion Pipeline\nWeb URL Data Ingestion Pipeline\nAzure Function App Ingestion Pipeline\nSharePoint Ingestion Pipeline\nDatabricks Processing Pipeline\nAzure Function App Processing Pipeline\nMultiple Azure Function Apps Processing Pipeline\nCopy File Processing Pipeline\nSQL Table Staging Pipeline\nMultiple SQL Table Staging Pipeline\n\n\n\n\n\n\n\nNote\n\n\n\nOriginally posted on the NHSX technical gateway website."
  },
  {
    "objectID": "posts/azure-data-factory-templates.html#sql-database-ingestion-pipeline",
    "href": "posts/azure-data-factory-templates.html#sql-database-ingestion-pipeline",
    "title": "Azure Data Factory Templates",
    "section": "SQL Database Ingestion Pipeline",
    "text": "SQL Database Ingestion Pipeline\n\nMetadata\n\nFILE: ingestion_sql.json\nDESCRIPTION: Pipeline to ingest raw data to Azure Datalake blob storage from a SQL database.\nCONTRIBUTORS: Craig Shenton, Mattia Ficarelli\nCONTACT: data@nhsx.nhs.uk\nCREATED: 20 Sept 2021\nVERSION: 0.0.1\n\n\n\nDescription\n\n\n\nData ingestion from a SQL database\n\n\nFigure 1: Data ingestion from a SQL database\nPipeline to ingest raw data to Azure Datalake blob storage from a SQL database.\n\nLooks up the .json configuration file for this pipeline\nSource:\n\nSets the source database owner (dbo)\nSets the source table\nSets the SQL query\n\nSink:\n\nSets the file system\nSets the sink path\nSets the sink file\n\nCopy activity copies the data returned from the SQL query as either a .csv file or a .parquet file.\nIf the copy activity fails, the error notification logic app API will notify the specified email address of the error\n\n\n\nPipeline Configuration\n{\n  \"pipeline\": {\n    \"name\": \"ingestion_sql\",\n    \"folder\": \"templates/ingestion/sql\",\n    \"adl_file_system\": \"file_system\",\n    \"raw\": {\n      \"source_dbo\": \"dbo\",\n      \"source_table\": \"table_1\",\n      \"source_query\": \"SELECT * FROM dbo.table_1 ORDER BY Date DESC\",\n      \"sink_path\": \"raw/path/to/data\",\n      \"sink_file\": \"table_1.parquet\"\n    }\n}\n\n\nData Factory Configuration\nDownload the Azure Data Factory json configuration file to use this template in your own data pipelines.\nsql-ingestion.json"
  },
  {
    "objectID": "posts/azure-data-factory-templates.html#databricks-ingestion-pipeline",
    "href": "posts/azure-data-factory-templates.html#databricks-ingestion-pipeline",
    "title": "Azure Data Factory Templates",
    "section": "Databricks Ingestion Pipeline",
    "text": "Databricks Ingestion Pipeline\n\nMetadata\n\nFILE: ingestion_databricks.json\nDESCRIPTION: Pipeline to ingest raw data to Azure Datalake blob storage using a databricks notebook.\nCONTRIBUTORS: Craig Shenton, Mattia Ficarelli\nCONTACT: data@nhsx.nhs.uk\nCREATED: 20 Sept 2021\nVERSION: 0.0.1\n\n\n\nDescription\n\n\n\nData ingestion using a databricks notebook\n\n\nFigure 2: Data ingestion using a databricks notebook\nPipeline to ingest raw data to Azure Datalake blob storage using a databricks notebook.\n\nLookup the .json configuration file for this pipeline.\nSet the databricks notebook path.\nDatabricks notebook activity runs the databricks notebook specified using an ephemeral job cluster.\nIf the databricks notebook activity fails, the error notification logic app API will notify the specified email address of the error.\n\nWithin the databricks notebook, using Azure Databricks Functions, data can be saved to blob storage as either a .csv file or a .parquet file.\n\n\nPipeline Configuration\n{\n  \"pipeline\": {\n    \"name\": \"ingestion_databricks\",\n    \"folder\": \"templates/ingestion/databricks\",\n    \"adl_file_system\": \"file_system\",\n    \"raw\": {\n      \"databricks_notebook\": \"/path/to/databricks/notebook\"\n    }\n}\n\n\nData Factory Configuration\nDownload the Azure Data Factory .json configuration file to use this template in your own data pipelines.\ndatabricks-ingestion.json"
  },
  {
    "objectID": "posts/azure-data-factory-templates.html#excel-sheet-ingestion-pipeline",
    "href": "posts/azure-data-factory-templates.html#excel-sheet-ingestion-pipeline",
    "title": "Azure Data Factory Templates",
    "section": "Excel Sheet Ingestion Pipeline",
    "text": "Excel Sheet Ingestion Pipeline\n\nMetadata\n\nFILE: ingestion_excel_sheet.json\nDESCRIPTION: Pipeline to ingest a specified excel file sheet, as a .csv file, to Azure Datalake blob storage.\nCONTRIBUTORS: Craig Shenton, Mattia Ficarelli\nCONTACT: data@nhsx.nhs.uk\nCREATED: 20 Sept 2021\nVERSION: 0.0.1\n\n\n\nDescription\n\n\n\nData ingestion of an excel file sheet\n\n\nFigure 3: Data ingestion of an excel file sheet\nPipeline to ingest a specified excel file sheet, as a .csv file, to Azure Datalake blob storage.\n\nLookup the .json configuration file for this pipeline.\nSet the Azure Datalake file system.\nSet the source file path, file name, and excel sheet name.\nSet the sink file path and file name.\nCopy activity ingests the excel sheet data to a .csv file.\nIf the copy activity fails, the error notification logic app API will notify the specified email address of the error.\n\n\n\nPipeline Configuration\n{\n  \"pipeline\": {\n    \"name\": \"ingestion_excel_sheet\",\n    \"folder\": \"templates/ingestion/excel_sheet\",\n    \"adl_file_system\": \"file_system\",\n    \"raw\": {\n      \"source_path\": \"raw/\",\n      \"source_file\": \"file.xlsx\",\n      \"source_sheet\": \"table_1\",\n      \"sink_path\": \"processed/\",\n      \"sink_name\": \"table_1.csv\"\n    }\n}\n\n\nData Factory Configuration\nDownload the Azure Data Factory .json configuration file to use this template in your own data pipelines. excel-sheet-ingestion.json\n\nNote\nAlternatively this a variation of this pipeline can be used to ingest multiple excel file sheets to a set of .csv files in Azure Datalake blob storage."
  },
  {
    "objectID": "posts/azure-data-factory-templates.html#multiple-excel-sheet-ingestion-pipeline",
    "href": "posts/azure-data-factory-templates.html#multiple-excel-sheet-ingestion-pipeline",
    "title": "Azure Data Factory Templates",
    "section": "Multiple Excel Sheet Ingestion Pipeline",
    "text": "Multiple Excel Sheet Ingestion Pipeline\n\nMetadata\n\nFILE: ingestion_multiple_excel_sheets.json\nDESCRIPTION: Pipeline to ingest multiple specified excel file sheets as .csv files to Azure Datalake blob storage.\nCONTRIBUTORS: Craig Shenton, Mattia Ficarelli\nCONTACT: data@nhsx.nhs.uk\nCREATED: 20 Sept 2021\nVERSION: 0.0.1\n\n\n\nDescription\n\n\n\nData ingestion of multiple excel file sheets\n\n\nFigure 4: Data ingestion of multiple excel file sheets\n\n\n\nForEach loop activities within pipeline\n\n\nFigure 5: ForEach loop activities within pipeline\nPipeline to ingest multiple specified excel file sheets as .csv files to Azure Datalake blob storage.\n\nLooks up the .json configuration file for this pipeline.\nSet the Azure Datalake file system.\nSet the source path to the folder containing the excel files.\nSet the sink path.\nSet an array variable containing the list of excel file metadata.\nForEach loops over each excel - FILE:\n\nSets the source sheet and sink file.\nCopy activity ingests the excel sheet data and saves it as a .csv file.\nIf the copy activity fails, the error notification logic app API will notify the specified email address of the error.\n\n\n\nNote\nCopy activity has ‘File path type’ set to wildcard and the file name regex as *.xlsx (excel) (see Figure 6).\n\n\n\nCopy activity wildcard setup\n\n\nFigure 6: Copy activity wildcard setup\n\n\n\nPipeline Configuration\n{\n  \"pipeline\": {\n    \"name\": \"ingestion_multiple_excel_sheets\",\n    \"folder\": \"templates/ingestion/multiple_excel_sheets\",\n    \"adl_file_system\": \"file_system\",\n    \"raw\": {\n      \"source_path\": \"ingestion/\",\n      \"sink_path\": \"raw/path/to/data\",\n      \"sink_path\": \"processed/\"\n      \"excel\":[\n    {\n      \"sink_file\": \"table_1.csv\",\n      \"source_sheet\": \"sheet_1\"\n    },\n    {\n      \"sink_file\": \"table_2.csv\",\n      \"source_sheet\": \"sheet_2\"\n    },\n    {\n      \"sink_file\": \"table_3.csv\",\n      \"source_sheet\": \"sheet_3\"\n    }\n  ]\n}\n\n\nData Factory Configuration\nDownload the Azure Data Factory .json configuration file to use this template in your own data pipelines.\nmultiple-excel-sheet-ingestion.json"
  },
  {
    "objectID": "posts/azure-data-factory-templates.html#web-url-data-ingestion-pipeline",
    "href": "posts/azure-data-factory-templates.html#web-url-data-ingestion-pipeline",
    "title": "Azure Data Factory Templates",
    "section": "Web URL Data Ingestion Pipeline",
    "text": "Web URL Data Ingestion Pipeline\n\nMetadata\n\nFILE: ingestion_web_url.json\nDESCRIPTION: Pipeline to ingest data from a URL as a .csv file to Azure Datalake blob storage.\nCONTRIBUTORS: Craig Shenton, Mattia Ficarelli\nCONTACT: data@nhsx.nhs.uk\nCREATED: 20 Sept 2021\nVERSION: 0.0.1\n\n\n\nDescription\n\n\n\nData ingestion from a web URL\n\n\nFigure 7: Data ingestion from a web URL\nPipeline to ingest data from a web URL as a .csv file to Azure Datalake blob storage.\n\nLookup the .json configuration file for this pipeline.\nSet the source URL.\nSet the file system.\nSet the sink path.\nSet the sink file.\nCopy activity copies the data returned from the URL as a .csv file.\nIf the copy activity fails, the error notification logic app API will notify the specified email address of the error.\n\n\n\nPipeline Configuration\n{\n  \"pipeline\": {\n    \"name\": \"ingestion_web_url\",\n    \"folder\": \"templates/ingestion/web_url\",\n    \"adl_file_system\": \"file_system\",\n    \"raw\": {\n      \"source_url\": \"https://www.sourcedata.com\",\n      \"sink_path\": \"raw/path/to/data\",\n      \"sink_file\": \"table_1.csv\"\n    }\n}\n\n\nData Factory Configuration\nDownload the Azure Data Factory .json configuration file to use this template in your own data pipelines.\nweb-url-ingestion.json"
  },
  {
    "objectID": "posts/azure-data-factory-templates.html#azure-function-app-ingestion-pipeline",
    "href": "posts/azure-data-factory-templates.html#azure-function-app-ingestion-pipeline",
    "title": "Azure Data Factory Templates",
    "section": "Azure Function App Ingestion Pipeline",
    "text": "Azure Function App Ingestion Pipeline\n\nMetadata\n\nFILE: ingestion_function_app.json\nDESCRIPTION: Pipeline to ingest raw data to Azure Datalake blob storage using an Azure function app.\nCONTRIBUTORS: Craig Shenton, Mattia Ficarelli\nCONTACT: data@nhsx.nhs.uk\nCREATED: 29 Sept 2021\nVERSION: 0.0.1\n\n\n\nDescription\n\n\n\nData ingestion using an azure function app\n\n\nFigure 8: Data ingestion using an azure function app\nPipeline to ingest raw data to Azure Datalake blob storage using an Azure function app.\n\nLookup the .json configuration file for this pipeline.\nSet the Azure function app.\nAzure function app activity triggers the specified function app.\nIf the Azure function app activity fails, the error notification logic app API will notify the specified email address of the error.\n\nWithin the Azure function app data can be saved to blob storage as either a .csv file or a .parquet file.\n\n\nPipeline Configuration\n{\n  \"pipeline\": {\n    \"name\": \"ingestion_function_app\",\n    \"folder\": \"templates/ingestion/function_app\",\n    \"adl_file_system\": \"file_system\",\n    \"raw\": {\n      \"func_name\": \"azure_func_app\"\n    }\n}\n\n\nData Factory Configuration\nDownload the Azure Data Factory .json configuration file to use this template in your own data pipelines.\nfunction-app-ingestion.json"
  },
  {
    "objectID": "posts/azure-data-factory-templates.html#sharepoint-ingestion-pipeline",
    "href": "posts/azure-data-factory-templates.html#sharepoint-ingestion-pipeline",
    "title": "Azure Data Factory Templates",
    "section": "SharePoint Ingestion Pipeline",
    "text": "SharePoint Ingestion Pipeline\n\nMetadata\n\nFILE: ingestion_sharepoint.json\nDESCRIPTION: Pipeline to ingest a specified folder and files from Microsoft SharePoint to Azure Datalake blob storage.\nCONTRIBUTORS: Craig Shenton, Mattia Ficarelli\nCONTACT: data@nhsx.nhs.uk\nCREATED: 29 Sept 2021\nVERSION: 0.0.1\n\n\n\nDescription\n\n\n\nData ingestion from microsoft sharepoint\n\n\nFigure 9: Data ingestion from microsoft sharepoint\nPipeline to ingest a specified folder from Microsoft SharePoint to Azure Datalake blob storage.\n\nLookup the .json configuration file for this pipeline.\nSet the SharePoint file path and SharePoint logic app URL.\nCall the SharePoint logic app using a webhook that will send back a message once the file transfer is complete.\nIf the logic app fails, the error notification logic app API will notify the specified email address of the error.\n\n\n\nPipeline Configuration\n{\n  \"pipeline\": {\n    \"name\": \"ingestion_sharepoint\",\n    \"folder\": \"templates/ingestion/sharepoint\",\n    \"adl_file_system\": \"file_system\",\n    \"raw\": {\n      \"source_path\": \"...sharepoint/...\",\n      \"logic_app_url\": \"https://...logic.azure.com/...\"\n    }\n}\n\n\nData Factory Configuration\nDownload the Azure Data Factory .json configuration file to use this template in your own data pipelines.\nsharepoint-ingestion.json"
  },
  {
    "objectID": "posts/azure-data-factory-templates.html#databricks-processing-pipeline",
    "href": "posts/azure-data-factory-templates.html#databricks-processing-pipeline",
    "title": "Azure Data Factory Templates",
    "section": "Databricks Processing Pipeline",
    "text": "Databricks Processing Pipeline\n\nMetadata\n\nFILE: processing_databricks.json\nDESCRIPTION: Pipeline to process data from a folder in Azure Datalake blob storage using a databricks notebook.\nCONTRIBUTORS: Craig Shenton, Mattia Ficarelli\nCONTACT: data@nhsx.nhs.uk\nCREATED: 23 Sept 2021\nVERSION: 0.0.1\n\n\n\nDescription\n\n\n\nData processing using a Databricks notebook\n\n\nFigure 10: Data processing using a Databricks notebook\nPipeline to process data from a folder in Azure Datalake blob storage using a databricks notebook\n\nLookup the .json configuration file for this pipeline.\nSet the databricks notebook path.\nDatabricks notebook activity runs the databricks notebook specified using an ephemeral job cluster.\nIf the databricks notebook activity fails, the error notification logic app API will notify the specified email address of the error.\n\n\n\nPipeline Configuration\n{\n  \"pipeline\": {\n    \"name\": \"processing_databricks\",\n    \"folder\": \"templates/processing/databricks\",\n    \"project\": {\n      \"databricks_notebook\": \"/path/to/databricks/notebook\"\n    }\n}\n\n\nDatabricks Orchestration\n\nNote\nAlternatively this pipeline can be used to trigger an orchestrator databricks notebook which in turn runs a series of data processing notebooks.\n{\n  \"pipeline\": {\n    \"name\": \"processing_databricks\",\n    \"folder\": \"templates/processing/databricks_orchestrator\",\n    \"project\": {\n      \"databricks_orchestrator_notebook\": \"/path/to/databricks/orchestrator_notebook\"\n      \"databricks\":[    \n          {\n        \"sink_path\": \"path/to/processed/data\",\n        \"sink_file\": \"file_1.csv\",\n        \"databricks_notebook\": \"/path/to/databricks/processing_notebook1\"\n        },    \n          {\n        \"sink_path\": \"path/to/processed/data\",\n        \"sink_file\": \"file_2.csv\",\n        \"databricks_notebook\": \"/path/to/databricks/processing_notebook2\"\n        },\n    }\n}\nPython code to sequentially run databricks notebook paths specified in a .json config file from a databricks orchestrator notebook.\n#Squentially run datbricks notebooks\nfor index, item in enumerate(config_JSON['pipeline']['project']['databricks']): \nnotebook = config_JSON['pipeline']['project']['databricks'][index]['databricks_notebook']\n    dbutils.notebook.run(notebook, 120)\n  except Exception as e:\n    print(e)\n\n\n\nData Factory Configuration\nDownload the Azure Data Factory .json configuration file to use this template in your own data pipelines.\nprocessing-databricks.json"
  },
  {
    "objectID": "posts/azure-data-factory-templates.html#azure-function-app-processing-pipeline",
    "href": "posts/azure-data-factory-templates.html#azure-function-app-processing-pipeline",
    "title": "Azure Data Factory Templates",
    "section": "Azure Function App Processing Pipeline",
    "text": "Azure Function App Processing Pipeline\n\nMetadata\n\nFILE: processing_function_app.json\nDESCRIPTION: Pipeline to process data to time-stamped folder in Azure Datalake blob storage using an Azure function app.\nCONTRIBUTORS: Craig Shenton, Mattia Ficarelli\nCONTACT: data@nhsx.nhs.uk\nCREATED: 29 Sept 2021\nVERSION: 0.0.1\n\n\n\nDescription\n\n\n\nData processing using an azure function app\n\n\nFigure 11: Data processing using an azure function app\n\nNote\nThis pipeline is designed to allow for raw data to be ingested and then appended onto an existing table with historical data.\nPipeline to process data to time-stamped folder in Azure Datalake blob storage using an Azure function app.\n\nLookup the .json configuration file for this pipeline.\nSet the source path (of the data to be processed).\nSet the file system.\nSet the Azure function app.\nUse the ‘laterFolder’ utility to find and save the latest folder in the source path.\nIf the ‘laterFolder’ utility fails, the error notification logic app API will notify the specified email address of the error.\nLookup the latest folder.\nSet the latest folder.\nSet the .json body for the Azure function app.\nRun the Azure function app activity.\nIf the Azure function app activity fails, the error notification logic app API will notify the specified email address of the error.\n\nWithin the Azure function app data can be saved to blob storage as either a .csv file or a .parquet file.\n\n\n\nPipeline Configuration\n{\n  \"pipeline\": {\n    \"name\": \"processing_function_app\",\n    \"folder\": \"templates/processing/function_app\",\n    \"adl_file_system\": \"file_system\",\n    \"project\": {\n      \"func_name\": \"azure_func_app\",\n      \"source_path\": \"raw/historical/data/source\"\n    }\n}\n\n\nData Factory Configuration\nDownload the Azure Data Factory .json configuration file to use this template in your own data pipelines.\nfunction-app-processing.json"
  },
  {
    "objectID": "posts/azure-data-factory-templates.html#multiple-azure-function-apps-processing-pipeline",
    "href": "posts/azure-data-factory-templates.html#multiple-azure-function-apps-processing-pipeline",
    "title": "Azure Data Factory Templates",
    "section": "Multiple Azure Function Apps Processing Pipeline",
    "text": "Multiple Azure Function Apps Processing Pipeline\n\nMetadata\n\nFILE: processing_multiple_function_apps.json\nDESCRIPTION: Pipeline to process data to time-stamped folders in Azure Datalake blob storage using multiple Azure function apps.\nCONTRIBUTORS: Craig Shenton, Mattia Ficarelli\nCONTACT: data@nhsx.nhs.uk\nCREATED: 29 Sept 2021\nVERSION: 0.0.1\n\n\n\nDescription\n\n\n\nData processing using multiple azure function apps\n\n\nFigure 12: Data processing using multiple azure function apps\n\n\n\nForEach loop activities within pipeline\n\n\nFigure 13: ForEach loop activities within pipeline\n\nNote\nThis pipeline allows for multiple different processed data files to be generated from the same data source during a pipeline run by using multiple function apps running sequentially.\nPipeline to process data to time-stamped folder in Azure Datalake blob storage using multiple Azure function apps.\n\nLookup the .json configuration file for this pipeline.\nSet the source path (of the data to be processed).\nSet the file system.\nSet the Azure function app.\nUse the ‘laterFolder’ utility to find and save the latest folder in the source path.\nIf the ‘laterFolder’ utility fails, the error notification logic app API will notify the specified email address of the error.\nLookup the latest folder.\nSet the latest folder.\nSet the .json body for the Azure function app.\nSet an array variable containing the list of Azure function apps to be run.\nForEach loops over each azure function: >\n\n\nRuns the Azure function app activity.\nIf the Azure function app activity fails, the error notification logic app API will notify the specified email address of the error.\n\nWithin the Azure function app data can be saved to blob storage as either a .csv file or a .parquet file.\n\n\n\nPipeline Configuration\n{\n  \"pipeline\": {\n    \"name\": \"processing_function_app\",\n    \"folder\": \"templates/processing/function_app\",\n    \"adl_file_system\": \"file_system\",\n    \"project\": {\n      \"functions\": [\n        {\"func_name\": \"azure_func_app_1\"},\n        {\"func_name\": \"azure_func_app_2\"},\n        {\"func_name\": \"azure_func_app_3\"}\n            ],\n      \"source_path\": \"raw/historical/data/source\"\n    }\n}\n\n\nData Factory Configuration\nDownload the Azure Data Factory .json configuration file to use this template in your own data pipelines.\nmultiple-function-app-processing.json"
  },
  {
    "objectID": "posts/azure-data-factory-templates.html#copy-file-processing-pipeline",
    "href": "posts/azure-data-factory-templates.html#copy-file-processing-pipeline",
    "title": "Azure Data Factory Templates",
    "section": "Copy File Processing Pipeline",
    "text": "Copy File Processing Pipeline\n\nMetadata\n\nFILE: processing_csv_file.json\nDESCRIPTION: Pipeline to copy a .csv file in a time-stamped folder between directories in Azure Datalake blob storage.\nCONTRIBUTORS: Craig Shenton, Mattia Ficarelli\nCONTACT: data@nhsx.nhs.uk\nCREATED: 29 Sept 2021\nVERSION: 0.0.1\n\n\n\nDescription\n\n\n\nCopying a .csv file between Azure Datalake directories\n\n\nFigure 14: Copying a .csv file between Azure Datalake directories\nPipeline to copy a .csv file in a time-stamped folder between directories in Azure Datalake blob storage.\n\nLookup the .json configuration file for this pipeline.\nSet the Azure Datalake file system\nSet the source path and source file name.\nSet the sink path and sink file name.\nUse the ‘laterFolder’ utility to find and save the latest folder in the source path.\nIf the ‘laterFolder’ utility fails, the error notification logic app API will notify the specified email address of the error.\nLookup the latest folder.\nSet the latest folder.\nCopy activity copies the .csv file between the Datalake directories.\nIf the copy activity fails, the error notification logic app API will notify the specified email address of the error.\n\n\n\nPipeline Configuration\n{\n  \"pipeline\": {\n    \"name\": \"processing_csv_file\",\n    \"folder\": \"templates/processing/csv_file\",\n    \"adl_file_system\": \"file_system\",\n    \"project\": {\n      \"source_path\": \"raw/\",\n      \"source_name\": \"file.csv\",\n      \"sink_path\": \"proc/\",\n      \"sink_name\": \"file_copy.csv\"\n    }\n}\n\n\nData Factory Configuration\nDownload the Azure Data Factory .json configuration file to use this template in your own data pipelines.\ncsv-file-processing.json"
  },
  {
    "objectID": "posts/azure-data-factory-templates.html#sql-table-staging-pipeline",
    "href": "posts/azure-data-factory-templates.html#sql-table-staging-pipeline",
    "title": "Azure Data Factory Templates",
    "section": "SQL Table Staging Pipeline",
    "text": "SQL Table Staging Pipeline\n\nMetadata\n\nFILE: staging_sql_database.json\nDESCRIPTION: Pipeline to stage data from a time-stamped folder in Azure Datalake blob storage to a table in an Azure SQL database.\nCONTRIBUTORS: Craig Shenton, Mattia Ficarelli\nCONTACT: data@nhsx.nhs.uk\nCREATED: 29 Sept 2021\nVERSION: 0.0.1\n\n\n\nDescription\n\n\n\nData staging to a table in an Azure SQL database\n\n\nFigure 15: Data staging to a table in an Azure SQL database\nPipeline to stage data (.csv file) from a time-stamped folder in Azure Datalake blob storage to a table in an Azure SQL database.\n\nLookup the .json configuration file for this pipeline.\nSet the source path (of data to be staged).\nSet the source file.\nSet the file system.\nSet the sink table (target table in the SQL database).\nSet the stored procedure (truncates data in the target table in the SQL database).\nRun the stored procedure activity. The stored procedure also sets the data type of each column in the database table.\nUse the ‘laterFolder’ utility to find and save the latest folder in the source path.\nIf the ‘laterFolder’ utility fails, the error notification logic app API will notify the specified email address of the error.\nLookup the latest folder.\nSet the latest folder.\nRun the copy activity which stages data from a .csv file in Azure Datalake blob storage to an empty table in an Azure SQL database.\nIf the Azure copy activity fails, the error notification logic app API will notify the specified email address of the error.\n\n\n\nPipeline Configuration\n{\n  \"pipeline\": {\n    \"name\": \"staging_sql_database\",\n    \"folder\": \"templates/staging/sql_database\",\n    \"adl_file_system\": \"file_system\",\n    \"staging\": {\n        \"stored_procedure\":\"[dbo].[sql_stored_procedure_table_1]\",\n        \"source_path\":\"proc/projects/path/to/processed/data/\",\n        \"source_file\":\"table_1.csv\",\n        \"sink_table\":\"sql_table_1\"\n    }\n}\n\n\nData Factory Configuration\nDownload the Azure Data Factory .json configuration file to use this template in your own data pipelines.\nsql-database-staging.json"
  },
  {
    "objectID": "posts/azure-data-factory-templates.html#multiple-sql-table-staging-pipeline",
    "href": "posts/azure-data-factory-templates.html#multiple-sql-table-staging-pipeline",
    "title": "Azure Data Factory Templates",
    "section": "Multiple SQL Table Staging Pipeline",
    "text": "Multiple SQL Table Staging Pipeline\n\nMetadata\n\nFILE: multiple_tables_staging_sql_database.json\nDESCRIPTION: Pipeline to stage data from a time-stamped folders in Azure Datalake blob storage to multiple tables in an Azure SQL database.\nCONTRIBUTORS: Craig Shenton, Mattia Ficarelli\nCONTACT: data@nhsx.nhs.uk\nCREATED: 29 Sept 2021\nVERSION: 0.0.1\n\n\n\nDescription\n\n\n\nData staging to multiple tables in an Azure SQL database\n\n\nFigure 16: Data staging to multiple tables in an Azure SQL database\n\n\n\nForEach loop activities within pipeline\n\n\nFigure 17: ForEach loop activities within pipeline\nPipeline to stage data (.csv files) from a time-stamped folders in Azure Datalake blob storage to multiple tables in an Azure SQL database.\n\nLookup the .json configuration file for this pipeline.\nSet the file system.\nSet an array variable containing the list of stored procedures and tables to which processed data is to be staged.\nFor each element in the list the ForEach loop:\n\nSets the source path (of data to be staged).\nSets the source file.\nUses the ‘laterFolder’ utility to find and save the latest folder in the source path.\nLookups the latest folder.\nSets the latest folder.\nSets the sink table (target table in the SQL database).\nSets the stored procedure (truncates data in the target table in the SQL database).\nRuns the stored procedure activity. The stored procedure also sets the data type of each column in the database table.\nRuns the copy activity which stages data from a .csv file in azure Datalake blob storage to an empty table in an Azure SQL database.\nIf the Azure copy activity fails, the error notification logic app API will notify the specified email address of the error.\n\n\n\n\nPipeline Configuration\n{\n  \"pipeline\": {\n    \"name\": \"multiple_tables_staging_sql_database\",\n    \"folder\": \"templates/staging/multiple_tables_sql_database\",\n    \"adl_file_system\": \"file_system\",\n    \"staging\": [\n          {\n            \"stored_procedure\":\"[dbo].[sql_stored_procedure_table_1]\",\n            \"source_path\":\"proc/projects/path/to/processed/data/\",\n            \"source_file\":\"table_1.csv\",\n            \"sink_table\":\"sql_table_1\"\n          },\n          {\n            \"stored_procedure\":\"[dbo].[sql_stored_procedure_table_2]\",\n            \"source_path\":\"proc/projects/path/to/processed/data2/\",\n            \"source_file\":\"table_2.csv\",\n            \"sink_table\":\"sql_table_2\"\n          },\n          {\n            \"stored_procedure\":\"[dbo].[sql_stored_procedure_table_3]\",\n            \"source_path\":\"proc/projects/path/to/processed/data3/\",\n            \"source_file\":\"table_3.csv\",\n            \"sink_table\":\"sql_table_3\"\n          }\n      ]\n}\n\n\nData Factory Configuration\nDownload the Azure Data Factory .json configuration file to use this template in your own data pipelines.\nmultiple-tables-sql-database-staging.json"
  },
  {
    "objectID": "posts/azure-data-engineering-principles.html",
    "href": "posts/azure-data-engineering-principles.html",
    "title": "Azure Data Engineering Principles",
    "section": "",
    "text": "Our principles are broken down into the following sections:\n\nAzure Data Engineering Principles\n\nParameterisation\n\nExample: latestFolder\n\nConfiguration-as-code\nStandardised ETL Design Patterns\n\nExample 1: SQL Database Ingestion Pipeline\nExample 2: Databricks Processing Pipeline\n\nHierarchical Pipeline Orchestration\nDocumentation-as-code\nReferences\n\n\n\n\n\n\n\n\nNote\n\n\n\nOriginally posted on the NHSX technical gateway website."
  },
  {
    "objectID": "posts/azure-data-engineering-principles.html#parameterisation",
    "href": "posts/azure-data-engineering-principles.html#parameterisation",
    "title": "Azure Data Engineering Principles",
    "section": "Parameterisation",
    "text": "Parameterisation\nIn straightforward copy activities, hard coding each activity’s file paths is easy enough. In Azure Data Factory (ADF), this requires creating a new dataset object for each sink and for each source. Like many users, we initially created new datasets at every stage of our Extract, Transform, and Load (ETL) pipelines. However, once these processes started to scale in complexity to include iteration and conditionals, the sheer amount of datasets and variables that were required to run our pipelines became unmanageable.\nThe first step in untangling this web of configurations is applying parameterisation to your data pipelines. This adds a layer of abstraction to ADF that can dramatically reduce the amount of complexity needed to handle a multitude of ETL processes. Parameterisation transforms your activities into something akin to a function in python that accepts a certain set of variables and arguments. Much like in python, this abstraction allows you to use and re-use the parameterised dataset for all processes of the same type, reducing the need to create a new dataset for each process.\nFor example, we created a generic dataset for handling .csv files on our Azure Datalake that passes the following parameters at runtime:\n\n\n\nFigure 1. An Azure Data Factory dataset file path configuration using the parameters; @dataset().fileSystem, @dataset().filePath, @dataset().fileName to denote the datalake file system name, the file path and and the file name.\n\n\nFrom these parameters, that specify the file path and name and the file system of the Azure Datalake linked service, we can use any .csv file available as the source for any pipeline activity. This has reduced the number of datasets listed in our ADF environment dramatically, reducing the overhead required to organise, search, and maintain our pipelines.\nA downside of highly parameterised pipelines is that they can become harder to debug due to the new level of abstraction. Now, in addition to the file paths the parameters may also be incorrectly configured. However, we find that the reduction in complexity and centralisation of pipeline configuration outweighs the initial growing pains of parameterisation.\n\nExample: latestFolder\n\n\n\nFigure 2. An example Azure Data Factory pipeline utility that can append the source path of any file with the latest time-stamped folder path.\n\n\nA practical example of the utility of parameterisation is the ability to append the source path of any file with a time-stamped folder, for example @concat(variables('sourcePath'),variables('latestFolder')). This allows for a well organised record of data sampled at different time points to be stored within the Datalake."
  },
  {
    "objectID": "posts/azure-data-engineering-principles.html#configuration-as-code",
    "href": "posts/azure-data-engineering-principles.html#configuration-as-code",
    "title": "Azure Data Engineering Principles",
    "section": "Configuration-as-code",
    "text": "Configuration-as-code\nConfiguration-as-code is the practice of managing the configuration of software using plain text .json files stored within a git repository. [2].\nThese ‘config’ files establish the parameters and settings for all of the datasets, linked services, and stored procedures required for a particular ETL pipeline. These files are called via ADF Lookup activities with values set as variables to give ADF everything it needs to know for a pipeline to run end-to-end. This approach means that in order to deploy a whole new data pipeline in ADF, only a new configuration file is required. Thus, in addition to making it easier and quicker to create a new ETL pipeline, maintaining configurations is also centralised, making configuration mismatches between activities easier to avoid, allowing for more consistent deployments.\nData Engineers often store their configurations in a database, alongside the data for convenience, however using structured .json files has additional advantages that should be considered:\n\nThe first is that as plain text files, they can be saved in a git repository, thus putting them under version control. This gives you a level of traceability in terms of how-and-when changes were made and allows for your configurations to go through the same DevOps best practices and code review before they are deployed to production [2].\nThe second benefit is that keeping configuration-as-code separates out your pipeline and configuration deployments. Decoupling these processes allows you to release and/or roll-back changes separately, which is important for tracing and debugging errors. Critically, this allows you to rapidly determine if the returned error is due to a configuration issue or a pipeline coding issue [3]."
  },
  {
    "objectID": "posts/azure-data-engineering-principles.html#standardised-etl-design-patterns",
    "href": "posts/azure-data-engineering-principles.html#standardised-etl-design-patterns",
    "title": "Azure Data Engineering Principles",
    "section": "Standardised ETL Design Patterns",
    "text": "Standardised ETL Design Patterns\nTemplates help us avoid building the same workflows repeatedly, as once developed and thoroughly tested, they can be used in many different pipelines. There are a growing number of common ETL templates available in ADF that are a great resource to get you started, found on Microsoft’s Azure documentation site [4].\nHowever, these templates still need to be hand configured for your specific pipeline. Applying the parameterisation and configuration-as-code principles outlined above to our templates allows us to go much further. We have created a set of fully abstract and perameratised ETL workflows that only require a configuration file lookup to run. In essence, these templates become ‘plug-and-play’, and can be chained together very quickly. By focusing on just a handful of generic and reusable templates, more resources can be allocated to testing and maintaining these resources, knowing that they will be used over and over, by many members of the development team. This is a far more efficient use of development time, and allows us to be confident that new pipelines will run upon their first implementation without much issue. Like the other components in ADF, our template files are simply stored as a JSON file within our code repository, so they can be shared and improved upon by the wider data engineering community.\nFor our internal analytics data engineering work, we have found it useful to break the templates into the following categories:\n\nIngestion: In the first instance we developed ingestion templates for every scenario, allowing us to rapidly ingest new datasets with minimal configuration. These typically involve HTTP requests, API calls, SQL stored procedures, and processes to copy files from SharePoint.\nProcessing: Our analytical processing is largely done through databricks, so these pipelines configure the analytics notebook and start a new spark job cluster.\nStaging: Staging is where we push data to our Tableau SQL server, so we have templates to run multiple stored procedures and update metric tables for each of our analytical products.\nUtilities: Last but by no means least, these are smaller functions that can be called multiple times at any stage of an ETL pipeline. Most involve sending data back and forth to systems outside ADF and/or updating configuration files.\n\n\nExample 1: SQL Database Ingestion Pipeline\n\n\n\nFigure 3. An example Azure Data Factory ingestion pipeline template that can be configured to extract data from an Azure SQL database to Azure Datalake blob storage.\n\n\nThe pipeline shown above is a fully parameterised template developed to ingest raw data from an Azure SQL database to Azure Datalake blob storage. This is an example of a ‘Source-Sink’ pattern–used for parameterising and configuring data copy activities that move data from one location to another. As mentioned in the parameterisation section, each Azure Data Factory copy activity requires at least two datasets to configure both source and sink locations. However, here we have created a generic SQL dataset and a generic Azure Datalake dataset that can be dynamically configured across all pipelines. As such, this template can be used and re-used to move data from any Azure SQL server to any Azure blob storage container.\nThe SQL ingestion template works as follows:\n\nThe configuration file is called using a lookup activity and the resulting configuration values are saved as variables before executing the copy activity at runtime.\nFor the source dataset, we require the parameters for connecting to an Azure SQL server.\nThe server details and connection credentials are passed via an ADF linked service, which itself can be further parameterised.\nThe configuration file then sets the source database owner (dbo) string, the source table name, and if required, a SQL query to filter the data before the copy activity is run.\nOn the sink side, the Datalake connection string is also passed via an ADF linked service, but the file system, sink path, and file name are all set by the configuration file. This could be a .csv file or a .parquet file for example\nFinally, if the copy activity fails for some reason, an error notification pipeline is called that contains a simple logic app used to notify a specified email address of the error [5].\n\n\n\nExample 2: Databricks Processing Pipeline\n\n\n\nFigure 4. An example Azure Data Factory pipeline processing pipeline template that can be configured to run a Databricks notebook.\n\n\nThe parameterised pipeline template above has been developed to run a Databricks notebook from Azure Data Factory. This is an example of a ‘Key-Value’ pattern–useful for configuring the settings of activities outside of Azure Data Factory itself. Here the json configuration file is providing the key-value of a Databricks notebook file path. This could also be used to give the URL of an Azure logic app or pass multiple variables to an Azure function app for example.\nThe Databricks processing template works as follows:\n\nThe configuration file is called using a lookup activity and the resulting configuration values are saved as variables before executing the copy activity at runtime.\nA Set variable activity reads the databricks notebook path and saves it as a Azure Data Factory variable.\nThe Databricks notebook activity runs the specified Databricks notebook using an ephemeral job cluster (therefore no cluster ID is required).\nFinally, if the Databricks notebook activity fails, an error notification pipeline is called that contains a simple logic app used to notify a specified email address of the error.\n\nWe typically use this pipeline to trigger an orchestrator Databricks notebook which in turn runs a series of data processing notebooks. This allows for much more flexibility, as we may want to process multiple metric calculations from the same data source."
  },
  {
    "objectID": "posts/azure-data-engineering-principles.html#hierarchical-pipeline-orchestration",
    "href": "posts/azure-data-engineering-principles.html#hierarchical-pipeline-orchestration",
    "title": "Azure Data Engineering Principles",
    "section": "Hierarchical Pipeline Orchestration",
    "text": "Hierarchical Pipeline Orchestration\n\n\n\nFigure 5.A hierarchicy of pipelines. At the top, a orchestration pipeline that triggers the sub-pipelines below. Each phase of the data processing (extract, transform, and load, or ETL) is a fully parameterised template that requires no setup of its own.\n\n\nOne of the most significant changes made to our data engineering setup is the use of hierarchical pipelines; that is the use of a single orchestration pipeline to trigger multiple ETL pipelines and utility sub-pipelines. This is based on Paul Andrews’ grandparent, parent, child design pattern that may be familiar to SSIS users [9]. With this design, and the generic ETL templates outlined above, we can create a ‘plug-and-play’ data engineering system. We simply select the ingestion, transformation, and staging patterns required from the templates and link them together under the orchestration pipeline. A single json config file is then created with all the ‘Sink-Source’ and ‘Key-Value’ pairs needed to move the data through the ETL process, so no configuration is required in Azure Data Factory itself. This has significantly reduced the amount of time needed to set up new pipelines, and ensures best practices are maintained across all our products.\nIf we use the SQL ingestion and Databricks processing examples outlined above, we could very rapidly make a new ETL pipeline using the hierarchical system of pipeline development:\n\nFirst we would create an orchestration pipeline that would lookup the configuration file and attach any Azure Data Factory triggers that set when, and how often the pipeline would run.\nNext we can simply ‘plug-and-play’ with the ETL templates, first to ingest new data from a SQL server, then process that data with a Databricks notebook.\nFinally, the processed data could be staged, on a Tableau server for example, for BI developers to transform into visualisations and metrics.\nWithin each ETL pipeline there would also be running pre-defined utility sub-pipelines, such as any testing and error handling processes, or our latestFolder example, which could be used to make sure we are processing the latest cut of the data before handing over to Databricks."
  },
  {
    "objectID": "posts/azure-data-engineering-principles.html#documentation-as-code",
    "href": "posts/azure-data-engineering-principles.html#documentation-as-code",
    "title": "Azure Data Engineering Principles",
    "section": "Documentation-as-code",
    "text": "Documentation-as-code\nDocumentation-as-code is the principle that either; documentation should be written with the same tools as your code, or that documentation should be automatically generated from your code [10]. On a basic level this means that we manage our documentation via GitHub, following a Git workflow, and putting it under version control in the same way as our configuration files. In addition to the standard GitFlow best practices, we can also compare versions of both the product and the documentation, making it easier to see where one might be out of sync with the other. Like much of our work, we make the documentation repository open source to increase transparency and allow for others in the healthcare sector to implement our data engineering principles and best practice. Due to the structured nature of the .json format, our pipeline configuration files and the configuration files generated from ADF are readily transformed into tables and diagrams using python. As a result, we can automatically generate a great deal of our documentation and make sure it directly represents the live product."
  },
  {
    "objectID": "posts/azure-data-engineering-principles.html#references",
    "href": "posts/azure-data-engineering-principles.html#references",
    "title": "Azure Data Engineering Principles",
    "section": "References",
    "text": "References\n[1] NHSX (2021). Data Engineering Documentation Site: ADF Utilities - latestFolder [Online] https://nhsx.github.io/au-data-engineering/adfutilities.html#latest-folder-lookup\n[2] Perforce (2020). Configuration as Code: How to Streamline Your Pipeline. [Online] https://www.perforce.com/blog/vcs/configuration-as-code\n[3] Cloud Bees (2018). Configuration as Code: Everything You Need to Know. [Online] https://www.cloudbees.com/blog/configuration-as-code-everything-need-know\n[4] Microsoft (2021). Azure Data Factory documentation site [Online] https://docs.microsoft.com/en-us/azure/data-factory/solution-templates-introduction\n[5] MSSQL Tips (2019). Azure Data Factory Pipeline Email Notification. [Online] https://www.mssqltips.com/sqlservertip/5718/azure-data-factory-pipeline-email-notification-part-1/\n[6] NHSX (2021). Data Engineering Documentation Site: SQL Database Ingestion Pipeline [Online] https://nhsx.github.io/au-data-engineering/adfpipelines.html#sql-database-ingestion-pipeline\n[7] NHSX (2021). Data Engineering Documentation Site: Databricks Processing Pipeline [Online] https://nhsx.github.io/au-data-engineering/adfpipelines.html#databricks-processing-pipeline\n[8] Paul Andrews (2019). The icons used for the hierarchical pipeline orchestration section of this post were designed by Paul Andrews. [Online] https://github.com/mrpaulandrew/ContentCollateral\n[9] Paul Andrews (2019). Azure Data Factory Pipeline Hierarchies (Generation Control) [Online] https://mrpaulandrew.com/2019/09/25/azure-data-factory-pipeline-hierarchies-generation-control/]\n[10] GOV.UK Technology in Government (2017). Why we use a ‘docs as code’ approach for technical documentation. [Online] https://technology.blog.gov.uk/2017/08/25/why-we-use-a-docs-as-code-approach-for-technical-documentation/\nChris Ried 2018. Cover photo from Unsplash. [Online] https://unsplash.com/s/photos/python?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText"
  },
  {
    "objectID": "posts/data_eng_list.html",
    "href": "posts/data_eng_list.html",
    "title": "Data Engineering Links",
    "section": "",
    "text": "Do You Need A Data Warehouse - A Quick Guide - Seattle Data Guy\nWhat’s a Data Vault Model and How to Implement It on the Databricks Lakehouse Platform - The Databricks Blog\nComposable OLAP\nQuestDB Fast SQL for time-series\nPrequel - Data syncs & data warehouse integration"
  },
  {
    "objectID": "posts/data_eng_list.html#projects",
    "href": "posts/data_eng_list.html#projects",
    "title": "Data Engineering Links",
    "section": "Projects",
    "text": "Projects\n\nProject A Data Modelling Best Practices Part I: How to Model Data in a Data Warehouse? - YouTube\n(500) Project A Data Modelling Best Practices Part II: How to Build a Data Warehouse? - YouTube\nDesigning a Data Project to Impress Hiring Managers - Start Data Engineering\nBuild Data Engineering Projects, with Free Template - Start Data Engineering\nData Engineering Project for Beginners - Batch edition - Start Data Engineering\nData engineering projects with template: Airflow, dbt, Docker, Terraform (IAC), Github actions (CI/CD) & more : dataengineering\nGitHub - DataTalksClub/data-engineering-zoomcamp: Free Data Engineering course!\nA data engineering project with Prefect, Docker, Terraform, Google CloudRun, BigQuery and Streamlit by Ryan Lamb Mar, 2023 Medium\neliasbenaddouidrissi\nGitHub - RSKriegs/finnhub-streaming-data-pipeline: Stream processing pipeline from Finnhub websocket using Spark, Kafka, Kubernetes and more\nGitHub - ankurchavda/streamify: A data engineering project with Kafka, Spark Streaming, dbt, Docker, Airflow, Terraform, GCP and much more!\nGitHub - ABZ-Aaron/Reddit-API-Pipeline\nGitHub - ris-tlp/audiophile-e2e-pipeline: Pipeline that extracts data from Crinacle’s Headphone and InEarMonitor databases and finalizes data for a Metabase Dashboard.\nGitHub - dominikhei/Local-Data-LakeHouse: Sample Data Lakehouse deployed in Docker containers using Apache Iceberg, Minio, Trino and a Hive Metastore. Can be used for local testing."
  },
  {
    "objectID": "posts/data_eng_list.html#pipelines",
    "href": "posts/data_eng_list.html#pipelines",
    "title": "Data Engineering Links",
    "section": "Pipelines",
    "text": "Pipelines\n\nData Pipeline Design Patterns - #1. Data flow patterns - Start Data Engineering\nstevecrox0914 comments on ETL using pandas\nMinimally Sufficient Pandas. In this article, I will offer an… by Ted Petrou Dunder Data Medium\n(756) Building a robust data pipeline with dbt, Airflow, and Great Expectations - YouTube\n\nReal-time Data Processing and Analysis with Kafka, Connect, KSQL, Elasticsearch, and Flask by Stefentaime Feb, 2023 Medium\n\n\nOrchestration\n\nLeverage Azure Databricks jobs orchestration from Azure Data Factory - Microsoft Community Hub\nIntroducing Databricks Workflows - The Databricks Blog\nDagster vs. Airflow Dagster Blog\n\n\n\nSpark\n\nIntroduction to Pyspark join types - Blog luminousmen\nThe 5-minute guide to using bucketing in Pyspark - Blog luminousmen\nSpark tips. DataFrame API - Blog luminousmen\nSpark core concepts explained - Blog luminousmen\n\n\n\ndbt\n\ndbt - Transform data in your warehouse\n(702) Introduction to dbt (data build tool) from Fishtown Analytics - YouTube\nLearn exactly how to use dbt™ in a modern data workflow Course\ndbt(Data Build Tool) Tutorial - Start Data Engineering\ncorp/dbt_style_guide.md at main - dbt-labs/corp - GitHub\nPopulate dbt models with CSV data. Part 2: the power of dbt-fal\nDBT Core & Airflow. Empowering our data organization with… by Albert Franzi Albert Franzi Medium\n\n\n\nKubernetes\n\nWhat is Kubernetes used for in data engineering? : dataengineering\n\n\n\nData Lake\n\nBig Data file formats - Blog luminousmen\nLakehouse — A resumé by Robert Kossendey claimsforce\nParquet: more than just “Turbo CSV”"
  },
  {
    "objectID": "posts/data_eng_list.html#testing",
    "href": "posts/data_eng_list.html#testing",
    "title": "Data Engineering Links",
    "section": "Testing",
    "text": "Testing\n\nCleaning sample data in standardized way Crystal Lewis\nThe Unreasonable Effectiveness of Data Pipeline Smoke Tests Dagster Blog\nIntroduction to the Pipeline Data Validation Workflow (VALID-II) - pointblank\nHow to Use Great Expectations in Databricks Great Expectations\nGitHub - gladykov/chain.train: ETL & database testing framework for big data, written with Python, using Pytest. With connectors for Spark, Snowflake, MySQL/MariaDB.\nTest data generation: There and back\nHow To Test Your Data With Great Expectations DigitalOcean\npytest tips and tricks pythontest"
  },
  {
    "objectID": "posts/data_eng_list.html#data-quality",
    "href": "posts/data_eng_list.html#data-quality",
    "title": "Data Engineering Links",
    "section": "Data Quality",
    "text": "Data Quality\n\nHow to use dbt-expectations to detect data quality issues\nData Quality Dimensions: Assuring Your Data Quality with Great Expectations - KDnuggets\nTest data quality at scale with Deequ AWS Big Data Blog\nDEEQU, I mean Data Quality. We say how we can turn the world upside… by Ajith Shetty Medium\nUnit Testing Data at Scale using Deequ and Apache Spark\nGuaranteeing Data Quality SLAs with Deequ & Databand - YouTube"
  },
  {
    "objectID": "posts/data_eng_list.html#cicd",
    "href": "posts/data_eng_list.html#cicd",
    "title": "Data Engineering Links",
    "section": "CI/CD",
    "text": "CI/CD\n\nE5: Building Better CI/CD Pipelines - by Marius Kimmina\nContinuous Delivery - YouTube\n2022 State of DevOps Report    Google Cloud\nContinuous Deployment or Continuous Delivery? When To Release - YouTube"
  },
  {
    "objectID": "posts/data_eng_list.html#documentation",
    "href": "posts/data_eng_list.html#documentation",
    "title": "Data Engineering Links",
    "section": "Documentation",
    "text": "Documentation\n\nThe importance of a handbook-first approach to documentation GitLab\nWhat is Continuous Documentation? The Manifesto Part I"
  },
  {
    "objectID": "posts/data_eng_list.html#managment",
    "href": "posts/data_eng_list.html#managment",
    "title": "Data Engineering Links",
    "section": "Managment",
    "text": "Managment\n\nModern data engineering playbook\nGitHub - andkret/Cookbook: The Data Engineering Cookbook\nData Team GitLab\nMy Facebook Bootcamp Experience\nWriting an engineering strategy. Irrational Exuberance\nThe Art and Science of Measuring Data Teams Value Airbyte\nDORA Four Key Metrics (Accelerate book) - Measuring Engineering Productivity\nAgile Is A Glass Cannon — Tom Dalling\nUsing architectural decision records to streamline technical decision-making for a software development project - AWS Prescriptive Guidance\nJsonnet - The Data Templating Language"
  },
  {
    "objectID": "posts/data_eng_list.html#observability",
    "href": "posts/data_eng_list.html#observability",
    "title": "Data Engineering Links",
    "section": "Observability",
    "text": "Observability\n\nIf data lineage is the answer, what is the question? by Gabs Ferreira Alvin Mar, 2023 Medium\nModern Data Strategy: Quality, Observability, Cataloging and Lineage by Danilo Drobac Feb, 2023 Medium"
  },
  {
    "objectID": "posts/data_eng_list.html#analytics",
    "href": "posts/data_eng_list.html#analytics",
    "title": "Data Engineering Links",
    "section": "Analytics",
    "text": "Analytics\n\nPouria Hadjibagheri and the UK’s abandoned open data revolution The Spectator\nHow The Post is replacing Mapbox with open source solutions - Kevin Schaul\nGitHub - protomaps/protomaps.js: Lightweight vector map rendering, labeling and symbology for the web"
  },
  {
    "objectID": "posts/data_eng_list.html#follow",
    "href": "posts/data_eng_list.html#follow",
    "title": "Data Engineering Links",
    "section": "Follow",
    "text": "Follow\n\n(756) Seattle Data Guy - YouTube\nDeep Channel - Tools for the data craft.\nAndreas Kretz - YouTube\nKahan Data Solutions - YouTube\nThe Developing Dev Ryan Peterman Substack\nData Engineering: Posts LinkedIn\nQuastor Quastor Tech Substack"
  },
  {
    "objectID": "posts/data_eng_list.html#career",
    "href": "posts/data_eng_list.html#career",
    "title": "Data Engineering Links",
    "section": "Career",
    "text": "Career\n\nA Senior Engineer’s Guide to the System Design Interview interviewing.io\nGitHub - jithendray/dp203-azure-data-engineering: notes for DP203-Data Engineering on Azure"
  },
  {
    "objectID": "posts/data_eng_list.html#data-science",
    "href": "posts/data_eng_list.html#data-science",
    "title": "Data Engineering Links",
    "section": "Data Science",
    "text": "Data Science\n\nFaker library in python - An intriguing expedient for data scientists by Sanjay Nandakumar Towards Data Science"
  },
  {
    "objectID": "posts/data_eng_list.html#security",
    "href": "posts/data_eng_list.html#security",
    "title": "Data Engineering Links",
    "section": "Security",
    "text": "Security\n\n5 Ways for Enterprise Teams to Secure Their DevOps Pipelines in 2023"
  },
  {
    "objectID": "posts/data_eng_list.html#python",
    "href": "posts/data_eng_list.html#python",
    "title": "Data Engineering Links",
    "section": "Python",
    "text": "Python\n\nAutomate your Python project with Makefile"
  },
  {
    "objectID": "posts/data_eng_list.html#sql",
    "href": "posts/data_eng_list.html#sql",
    "title": "Data Engineering Links",
    "section": "SQL",
    "text": "SQL\n\nHarold Sinnott #MWC23 on Twitter: “Best of #SQL Cheat Sheet\nSQLBolt - Learn SQL - Introduction to SQL"
  },
  {
    "objectID": "posts/data_eng_list.html#learn",
    "href": "posts/data_eng_list.html#learn",
    "title": "Data Engineering Links",
    "section": "Learn",
    "text": "Learn\n\nExam DP-203: Data Engineering on Microsoft Azure - Certifications Microsoft Learn\nLearn MS SQL Server & PostgreSQL: Database Design A-Z™ Udemy\nUltimate AWS Certified Solutions Architect Associate (SAA) Udemy\nThe Complete SQL Bootcamp for the Manipulation and Analysis of Data Udemy\nLinux for Beginners: Linux Basics Udemy\nData Warehouse Fundamentals for Beginners Udemy\nSpark and Python for Big Data with PySpark Udemy\nThe Complete Hands-On Introduction to Apache Airflow Udemy\n[8 Course BUNDLE]: DP-203: Data Engineering on MS Azure Udemy\nBI Analysis: MySQL for Data Analytics and Business Intelligence Udemy\nDocker & Kubernetes: The Practical Guide- [2023 Edition] Udemy\nData Warehouse - The Ultimate Guide Udemy\nPractice Exams AWS Certified Solutions Architect Associate Udemy"
  },
  {
    "objectID": "posts/data_eng_list.html#misc",
    "href": "posts/data_eng_list.html#misc",
    "title": "Data Engineering Links",
    "section": "Misc",
    "text": "Misc\n\nWhat is Data Engineering? - The Pragmatic Engineer\nIf You Only Read A Few Data Articles In 2023, Read These by Sven Balnojan Geek Culture Feb, 2023 Medium\nLow Code Software Development Is A Lie\nblog/we-need-a-new-versioning-notation.md at main - davidbullado/blog - GitHub"
  },
  {
    "objectID": "posts/data_eng_list.html#uncategorised-todo",
    "href": "posts/data_eng_list.html#uncategorised-todo",
    "title": "Data Engineering Links",
    "section": "Uncategorised (todo)",
    "text": "Uncategorised (todo)\n\nWelcome Superset\nLibrary Genesis\nApache Iceberg\nconfusables - PyPI\nfal - Features & Labels\nEarthly CI: Launching a new era for CI - Earthly Blog\nPricing Tabular\npydantic-factories - PyPI\nVault by HashiCorp\ndigitalghost-dev (Christian) - GitHub\nGitHub - andrem8/surf_dash\nGitHub - context-labs/autodoc: Toolkit for auto-generating codebase documentation using LLMs\nJsonnet - The Data Templating Language\nMountpoint for S3 : dataengineering\nData Quality Dimensions: Assuring Your Data Quality with Great Expectations - KDnuggets\nSQLMesh\nComparisons - SQLMesh\nSoda Core - Data Reliability Engineering as Code\nHow to Join a fact and a type 2 dimension (SCD2) table - Start Data Engineering\nPerspective Perspective\n5 Helpful Extract & Load Practices for High-Quality Raw Data by Sven Balnojan Apr, 2023 Towards Data Science\nCDC in Production: An Operating Guide\nImplementing Data Contracts at GoCardless Tech @ GoCardless\nA complete Apache Airflow tutorial: building data pipelines with Python AI Summer\nGitHub - getdozer/dozer: Connect any data source, combine them in real-time and instantly get low-latency Data APIs. All with just a simple configuration!\nDozer Start building real-time data apps in minutes Dozer Start building real-time data apps in minutes\nDevOps uses a capability model, not a maturity model - Octopus Deploy\nddanieltan.com - How to add some personality to your Quarto Blog\nTech Dive - APIs - by Quastor Tech - Quastor\nTech Dive - APIs PART 2 - by Quastor Tech - Quastor\nTech Dive - Database Sharding - by Quastor Tech - Quastor\nLearn System Design? - by Quastor Tech - Quastor\nThe Metaverse - by Quastor Tech - Quastor\nSoftware Architecture in the Real World - by Quastor Tech\nDocker Explained - by Quastor Tech - Quastor\nThe Architecture Behind the World’s Largest Developer Site…\nTech Dive - APIs - by Quastor Tech - Quastor\nThe Engineering Behind Facebook Newsfeed\nDistributed Databases Explained - by Quastor Tech\nKhan Academy’s migration from Python to Go\nA summary of Clean Code - by Quastor Tech - Quastor\nClean Code - Part 2 - by Quastor Tech - Quastor\nClean Code - Part 3 - by Quastor Tech - Quastor\nHow WhatsApp scaled to 1 billion users with only 50 engineers\nThe Architecture of Databases - by Quastor Tech - Quastor\nPrinciples for API Design - by Quastor Tech - Quastor\nHow Notion sharded their Postgres Database\nHow GitHub shifted from a Monolith to Microservices\nPartitioning Relational Databases at GitHub\nSoftware Architecture Principles - by Quastor Tech\nAn Introduction to Big Data Architectures - by Quastor Tech\nWhy LinkedIn changed their tech stack - by Quastor Tech\nHow Khan Academy Rewrote their Backend - by Quastor Tech\nHow Distributed Databases work - by Quastor Tech - Quastor\nClean Code’s Advice on Comments - by Quastor Tech - Quastor\nThe Architecture of Databases - by Quastor Tech - Quastor\nObservability at Twitter - by Quastor Tech - Quastor\nAirbnb’s Architecture - by Quastor Tech - Quastor\nHow to Design Better APIs - by Quastor Tech - Quastor\nReliability Engineering at BlackRock - by Quastor Tech\nDesign Docs at Google\nHow Notion Sharded Their Postgres Database\nHow WhatsApp scaled to 1 billion users with only 50 engineers\nExplaining the mechanics of Spark caching - Blog luminousmen\nChoosing the Right AWS Storage Service: A Comprehensive Guide to S3, S3N, and S3A - Blog luminousmen\nMaking CI workflow faster with Github Actions - Blog luminousmen\nQuality is the responsibility of the team - Blog luminousmen\nRumbling about Test Driven Development - Blog luminousmen\nDrunk Post: Things I’ve learned as a Sr Engineer- [reddit] - Blog luminousmen\nFirst rule of the leader - grow people - Blog luminousmen\nChange Data Capture (CDC) - Blog luminousmen\nACID vs BASE: Comparison of two Design Philosophies - Blog luminousmen\nOperational Challenges in Big Data - Blog luminousmen\nAnalytical Challenges in Big Data - Blog luminousmen\nArchitecturally Significant Requirements - Blog luminousmen\nManagement Challenges in Big Data - Blog luminousmen\nData Challenges in Big Data - Blog luminousmen\nSoft Skills guide for Software Engineer - Blog luminousmen\nModern Big Data Architectures - Lambda & Kappa - Blog luminousmen\nGuidelines for business meetings - Blog luminousmen\nAirflow dag dependencies - Blog luminousmen\nClean up your digital hygiene - Blog luminousmen\nData Engineering skills - Blog luminousmen\nKubernetes 101 - Blog luminousmen\nData Lake vs Data Warehouse - Blog luminousmen\nPython resource limitation - Blog luminousmen\nThe ultimate Python style guidelines - Blog luminousmen\nStay SaaSy Keeping it SaaSy\nSLS Letter from Dean"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Craig Robert Shenton",
    "section": "",
    "text": "Data Engineering Links\n\n\n\n\nData Engineering key concepts, tools, and best practices.\n\n\n\n\n\n\n4/8/23\n\n\n\n\n\n\n\n\nAzure Data Factory Templates\n\n\n\n\nOpen access and reusable design documentation of pipelines used in the NHSX Analytics Azure Data Factory (ADF) environment.\n\n\n\n\n\n\n9/9/21\n\n\n\n\n\n\n\n\nAzure Data Engineering Principles\n\n\n\n\nIn this post, we aim to set out the data engineering principles NHSX has developed in designing and building our Azure cloud-analytics infrastructure.\n\n\n\n\n\n\n9/1/21\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "drafts/azure-data-factory-resources.html",
    "href": "drafts/azure-data-factory-resources.html",
    "title": "Azure Data Factory Resources",
    "section": "",
    "text": "Transforming JSON to CSV with the help of Flatten task in Azure Data Factory\n{:class=“img-responsive”}\nTransform/transpose/flatten your JSON structure into a denormalized flatten datasets that you can upload into a new or existing flat database table.\nHow to read files on SharePoint Online using Azure Data Factory. Organizations that have moved to Office 365, maintain few key data points on excel files/SharePoint Lists stored in SharePoint Online. If the situation demands you to analyze these data points, it has to be consumed to a database or a data lake.\nAzure Data Factory Pipeline Email Notification – Part 1. When building ETL pipelines, you typically want to notify someone when something goes wrong (or when everything has finished successfully). Usually this is done by sending an e-mail to the support team or someone else who is responsible for the ETL. But in Azure Data Factory there’s no built-in activity for sending an e-mail. Here is an implementation using the Web Activity and an Azure Logic App."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Craig Robert Shenton",
    "section": "",
    "text": "Hi 👋, I’m Craig."
  }
]